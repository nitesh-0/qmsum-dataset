[
    {
        "transcript": "It 's not very significant . Uh , channel one . Yes . Channel three . OK . Mm - hmm . Channel three . Ta Channel three . Alright . OK , did you solve speech recognition last week ? Almost . Alright ! Let 's do image processing . Yes , again . Great . We did it again , Morgan . Alright ! Doo - doop , doo - doo . What 's wrong with {disfmarker} ? OK . It 's April fifth . Actually , Hynek should be getting back in town shortly if he isn't already . Is he gonna come here ? Uh . Well , we 'll drag him here . I know where he is . So when you said \" in town \" , you mean {pause} Oregon . U u u u uh , I meant , you know , this end of the world , yeah , {vocalsound} is really what I meant , Oh . Doo , doo - doo . uh , cuz he 's been in Europe . Doo - doo . So . I have something just fairly brief to report on . Mmm . Um , I did some {pause} experim uh , uh , just a few more experiments before I had to , {vocalsound} uh , go away for the w well , that week . Great ! Was it last week or whenever ? Um , so what I was started playing with was the {disfmarker} th again , this is the HTK back - end . And , um , I was curious because the way that they train up the models , {vocalsound} they go through about four sort of rounds of {disfmarker} of training . And in the first round they do {disfmarker} uh , I think it 's three iterations , and for the last three rounds e e they do seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {disfmarker} the {disfmarker} the back - end for this . I 'm sorry , I didn't quite get that . There 's {disfmarker} there 's four and there 's seven and {disfmarker} I {disfmarker} I 'm sorry . Yeah . Uh , maybe I should write it on the board . So , {vocalsound} there 's four rounds of training . Um , I g I g I guess you could say iterations . The first one is three , then seven , seven , and seven . And what these numbers refer to is the number of times that the , uh , HMM re - estimation is run . It 's this program called H E But in HTK , what 's the difference between , uh , a {disfmarker} an inner loop and an outer loop in these iterations ? OK . So what happens is , um , at each one of these points , you increase the number of Gaussians in the model . Yeah . Oh , right ! This was the mix up stuff . Yeah . The mix up . That 's right . Right . I remember now . And so , in the final one here , you end up with , uh {disfmarker} for all of the {disfmarker} the digit words , you end up with , uh , three {pause} mixtures per state , Yeah . eh , in the final {pause} thing . So I had done some experiments where I was {disfmarker} I {disfmarker} I want to play with the number of mixtures . Mm - hmm . But , um , uh , I wanted to first test to see if we actually need to do {pause} this many iterations early on . Uh , one , two , Mm - hmm . And so , um , I {disfmarker} I ran a couple of experiments where I {vocalsound} reduced that to l to be three , two , two , {vocalsound} uh , five , I think , and I got almost the exact same results . Mm - hmm . And {disfmarker} but it runs much much faster . So , um , I {disfmarker} I think m {pause} it only took something like , uh , three or four hours to do the full training , As opposed to {disfmarker} ? Good . as opposed to wh what , sixteen hours or something like that ? I mean , it takes {disfmarker} you have to do an overnight basically , the way it is set up now . Yeah . It depends . Mm - hmm . Mm - hmm . So , uh , even we don't do anything else , doing something like this could allow us to turn experiments around a lot faster . And then when you have your final thing , do a full one , so it 's {disfmarker} And when you have your final thing , we go back to this . Yeah . So , um , and it 's a real simple change to make . I mean , it 's like one little text file you edit and change those numbers , and you don't do anything else . Oh , this is a {disfmarker} Mm - hmm . And then you just run . OK . So it 's a very simple change to make and it doesn't seem to hurt all that much . So you {disfmarker} you run with three , two , two , five ? That 's a So I {disfmarker} Uh , I {disfmarker} I have to look to see what the exact numbers were . Yeah . I {disfmarker} I thought was , like , three , two , two , five , Mm - hmm . but I I 'll {disfmarker} I 'll double check . It was {vocalsound} over a week ago that I did it , OK . Mm - hmm . so I can't remember exactly . Oh . But , uh {disfmarker} Mm - hmm . um , but it 's so much faster . I it makes a big difference . Hmm . So we could do a lot more experiments and throw a lot more stuff in there . Yeah . That 's great . Um . Oh , the other thing that I did was , um , {vocalsound} I compiled {pause} the HTK stuff for the Linux boxes . So we have this big thing that we got from IBM , which is a five - processor machine . Really fast , but it 's running Linux . So , you can now run your experiments on that machine and you can run five at a time and it runs , {vocalsound} uh , as fast as , you know , uh , five different machines . Mm - hmm . Mm - hmm . So , um , I 've forgotten now what the name of that machine is but I can {disfmarker} I can send email around about it . Yeah . And so we 've got it {disfmarker} now HTK 's compiled for both the Linux and for , um , the Sparcs . Um , you have to make {disfmarker} you have to make sure that in your dot CSHRC , {vocalsound} um , it detects whether you 're running on the Linux or a {disfmarker} a Sparc and points to the right executables . Uh , and you may not have had that in your dot CSHRC before , if you were always just running the Sparc . So , um , Mm - hmm . uh , I can {disfmarker} I can tell you exactly what you need to do to get all of that to work . But it 'll {disfmarker} it really increases what we can run on . Hmm . Cool . So , {vocalsound} together with the fact that we 've got these {pause} faster Linux boxes and that it takes less time to do {pause} these , um , we should be able to crank through a lot more experiments . Mm - hmm . So . Hmm . So after I did that , then what I wanted to do {comment} was try {pause} increasing the number of mixtures , just to see , um {disfmarker} see how {disfmarker} how that affects performance . Yeah . So . Yeah . In fact , you could do something like {pause} keep exactly the same procedure and then add a fifth thing onto it Mm - hmm . that had more . Exactly . Yeah . Right . Right . So at {disfmarker} at the middle o where the arrows are showing , that 's {disfmarker} you 're adding one more mixture per state , Uh - huh . Uh , or {disfmarker} ? let 's see , uh . It goes from this {disfmarker} uh , try to go it backwards {disfmarker} this {disfmarker} at this point it 's two mixtures {pause} per state . So this just adds one . Except that , uh , actually for the silence model , it 's six mixtures per state . Mm - hmm . Uh , so it goes to two . OK . Um . And I think what happens here is {disfmarker} Might be between , uh , shared , uh {disfmarker} shared variances or something , Yeah . I think that 's what it is . or {disfmarker} Uh , yeah . It 's , uh {disfmarker} Shoot . I {disfmarker} I {disfmarker} I can't remember now what happens at that first one . Uh , I have to look it up and see . Oh , OK . Um , there {disfmarker} because they start off with , uh , an initial model which is just this global model , and then they split it to the individuals . And so , {vocalsound} it may be that that 's what 's happening here . I {disfmarker} I {disfmarker} {vocalsound} I have to look it up and see . I {disfmarker} I don't exactly remember . OK . OK . So . That 's it . Alright . So what else ? Um . Yeah . There was a conference call this Tuesday . Um . I don't know yet the {disfmarker} {vocalsound} what happened {vocalsound} Tuesday , but {vocalsound} the points that they were supposed to discuss is still , {vocalsound} uh , things like {vocalsound} the weights , uh {disfmarker} Oh , this is a conference call for , uh , uh , Aurora participant sort of thing . For {disfmarker} Yeah . Yeah . I see . Mmm . Do you know who was {disfmarker} who was {disfmarker} since we weren't in on it , uh , do you know who was in from OGI ? Was {disfmarker} {vocalsound} was {disfmarker} was Hynek involved or was it Sunil I have no idea . or {disfmarker} ? Mmm , I just {disfmarker} Oh , you don't know . OK . Yeah . Alright . Um , yeah . So the points were the {disfmarker} the weights {disfmarker} how to weight the different error rates {vocalsound} that are obtained from different language and {disfmarker} and conditions . Um , it 's not clear that they will keep the same kind of weighting . Right now it 's a weighting on {disfmarker} on improvement . Mm - hmm . Some people are arguing that it would be better to have weights on uh {disfmarker} well , to {disfmarker} to combine error rates {pause} before computing improvement . Uh , and the fact is that for {disfmarker} right now for {pause} the English , they have weights {disfmarker} they {disfmarker} they combine error rates , but for the other languages they combine improvement . So it 's not very consistent . Um {disfmarker} Mm - hmm . Yeah . The , um {disfmarker} Yeah . And so {disfmarker} Well , {vocalsound} this is a point . And right now actually there is a thing also , {vocalsound} uh , that happens with the current weight is that a very non - significant improvement {pause} on the well - matched case result in {pause} huge differences in {disfmarker} {vocalsound} in the final number . Mm - hmm . And so , perhaps they will change the weights to {disfmarker} Hmm . Yeah . How should that be done ? I mean , it {disfmarker} it seems like there 's a simple way {disfmarker} Mm - hmm . Uh , this seems like an obvious mistake or something . Well , I mean , the fact that it 's inconsistent is an obvious mistake . Th - they 're {disfmarker} But the {disfmarker} but , um , the other thing {disfmarker} In I don't know I haven't thought it through , but one {disfmarker} one would think that {vocalsound} each {disfmarker} It {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ? Mm - hmm . It depends what you wanna show . Mm - hmm . Each {disfmarker} each one is gonna have a different characteristic . Yeah . So {disfmarker} Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement . Tha - that 's what they do . Well , they are doing that . Yeah . No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error rates and take the relative improvement maybe of that ? Yeah . Yeah . And the thing is it 's not just a pure average because there are these weightings . Oh . It 's a weighted average . Um . Yeah . And so when you average the {disfmarker} the relative improvement it tends to {disfmarker} {vocalsound} to give a lot of {disfmarker} of , um , {vocalsound} importance to the well - matched case because {pause} the baseline is already very good and , um , i it 's {disfmarker} Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores Mm - hmm . with a weight or whatever , and then give you a score {disfmarker} here 's your score . And then they can do the same thing for the baseline system {disfmarker} and here 's its score . And then you can look at {disfmarker} Mm - hmm . Well , that 's what he 's seeing as one of the things they could do . Yeah . It 's just when you {disfmarker} when you get all done , I think that they pro I m I {disfmarker} I wasn't there but I think they started off this process with the notion that {vocalsound} you should be {pause} significantly better than the previous standard . Mm - hmm . And , um , so they said \" how much is significantly better ? what do you {disfmarker} ? \" And {disfmarker} and so they said \" well , {vocalsound} you know , you should have half the errors , \" or something , \" that you had before \" . Mm - hmm . Hmm . Mm - hmm . Yeah . So it 's , uh , But it does seem like Hmm . i i it does seem like it 's more logical to combine them first and then do the {disfmarker} Combine error rates and then {disfmarker} Yeah . Yeah . Well {disfmarker} Yeah . But there is this {disfmarker} this {disfmarker} is this still this problem of weights . When {disfmarker} when you combine error rate it tends to {pause} give more importance to the difficult cases , and some people think that {disfmarker} Oh , yeah ? well , they have different , {vocalsound} um , opinions about this . Some people think that {vocalsound} it 's more important to look at {disfmarker} {vocalsound} to have ten percent imp relative improvement on {pause} well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch and {disfmarker} So , bu It sounds like they don't really have a good idea about what the final application is gonna be . l de fff ! Mmm . Well , you know , the {disfmarker} the thing is {vocalsound} that if you look at the numbers on the {disfmarker} on the more difficult cases , {vocalsound} um , if you really believe that was gonna be the predominant use , {vocalsound} none of this would be good enough . Yeah . Mmm . Yeah . Nothing anybody 's {disfmarker} Mm - hmm . whereas {vocalsound} you sort of with some reasonable error recovery could imagine in the better cases that these {disfmarker} these systems working . So , um , I think the hope would be that it would {disfmarker} {vocalsound} uh , it would work well {pause} for the good cases and , uh , it would have reasonable {disfmarker} reas {vocalsound} soft degradation as you got to worse and worse conditions . Um . Yeah . I {disfmarker} I guess what I 'm {disfmarker} I mean , I {disfmarker} I was thinking about it in terms of , if I were building the final product and I was gonna test to see which front - end I 'd {disfmarker} {vocalsound} I wanted to use , I would {vocalsound} try to {pause} weight things depending on the exact environment that I was gonna be using the system in . But {disfmarker} but {disfmarker} No . If I {disfmarker} Well , no {disfmarker} well , no . I mean , {vocalsound} it isn't the operating theater . I mean , they don they {disfmarker} they don't {disfmarker} they don't really {pause} know , I think . Yeah . I mean , I th So if {disfmarker} if they don't know , doesn't that suggest the way for them to go ? Uh , you assume everything 's equal . I mean , y y I mean , you {disfmarker} Well , I mean , I {disfmarker} I think one thing to do is to just not rely on a single number {disfmarker} to maybe have two or three numbers , Yeah . you know , Right . and {disfmarker} and {disfmarker} and say {vocalsound} here 's how much you , uh {disfmarker} you improve {vocalsound} the , uh {disfmarker} the {disfmarker} the relatively clean case and here 's {disfmarker} or {disfmarker} or well - matched case , and here 's how {disfmarker} here 's how much you , Mm - hmm . uh {disfmarker} So not {disfmarker} So . So not try to combine them . Yeah . Uh , actually it 's true . Yeah . Uh , I had forgotten this , uh , but , uh , well - matched is not actually clean . What it is is just that , u uh , the training and testing are similar . The training and testing . Mmm . So , I guess what you would do in practice is you 'd try to get as many , {vocalsound} uh , examples of similar sort of stuff as you could , and then , Yeah . uh {disfmarker} So the argument for that being the {disfmarker} the {disfmarker} the more important thing , {vocalsound} is that you 're gonna try and do that , {vocalsound} but you wanna see how badly it deviates from that when {disfmarker} when {disfmarker} when the , uh {disfmarker} it 's a little different . So {disfmarker} Um , so you should weight those other conditions v very {disfmarker} you know , really small . But {disfmarker} No . That 's a {disfmarker} that 's a {disfmarker} that 's an arg I mean , that 's more of an information kind of thing . that 's an ar Well , that 's an argument for it , but let me give you the opposite argument . The opposite argument is you 're never really gonna have a good sample of all these different things . Uh - huh . I mean , are you gonna have w uh , uh , examples with the windows open , half open , full open ? Going seventy , sixty , fifty , forty miles an hour ? On what kind of roads ? Mm - hmm . With what passing you ? With {disfmarker} uh , I mean , Mm - hmm . I {disfmarker} I {disfmarker} I think that you could make the opposite argument that the well - matched case is a fantasy . Mm - hmm . You know , so , Uh - huh . I think the thing is is that if you look at the well - matched case versus the po you know , the {disfmarker} the medium and the {disfmarker} and the fo and then the mismatched case , {vocalsound} um , we 're seeing really , really big differences in performance . Right ? And {disfmarker} and y you wouldn't like that to be the case . You wouldn't like that as soon as you step outside {disfmarker} You know , a lot of the {disfmarker} the cases it 's {disfmarker} is {disfmarker} Well , that 'll teach them to roll their window up . I mean , in these cases , if you go from the {disfmarker} the , uh {disfmarker} I mean , I don't remember the numbers right off , but if you {disfmarker} if you go from the well - matched case to the medium , {vocalsound} it 's not an enormous difference in the {disfmarker} in the {disfmarker} the training - testing situation , and {disfmarker} and {disfmarker} and it 's a really big {vocalsound} performance drop . Mm - hmm . You know , so , um {disfmarker} Yeah , I mean the reference one , for instance {disfmarker} this is back old on , uh {disfmarker} on Italian {disfmarker} uh , was like {pause} six percent error for the well - matched and eighteen for the medium - matched and sixty for the {disfmarker} {vocalsound} for highly - mismatched . Uh , and , you know , with these other systems we {disfmarker} we {vocalsound} helped it out quite a bit , but still there 's {disfmarker} there 's something like a factor of two or something between well - matched and medium - matched . And {vocalsound} so I think that {vocalsound} if what you 're {disfmarker} {vocalsound} if the goal of this is to come up with robust features , it does mean {disfmarker} So you could argue , in fact , that the well - matched is something you shouldn't be looking at at all , that {disfmarker} that the goal is to come up with features {vocalsound} that will still give you reasonable performance , you know , with again gentle degregra degradation , um , even though the {disfmarker} the testing condition is not the same as the training . Hmm . So , you know , I {disfmarker} I could argue strongly that something like the medium mismatch , which is you know not compl pathological but {disfmarker} I mean , what was the {disfmarker} the medium - mismatch condition again ? Um , {vocalsound} it 's {disfmarker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {disfmarker} or {pause} stopped car and tested on {pause} high - speed conditions , I think , like on a highway and {disfmarker} Right . So {disfmarker} So it 's still the same {disfmarker} same microphone in both cases , Same microphone but {disfmarker} Yeah . but , uh , it 's {disfmarker} there 's a mismatch between the car conditions . And that 's {disfmarker} uh , you could argue that 's a pretty realistic situation Yeah . Mm - hmm . and , uh , I 'd almost argue for weighting that highest . But the way they have it now , {vocalsound} it 's {disfmarker} I guess it 's {disfmarker} it 's {disfmarker} They {disfmarker} they compute the relative improvement first and then average that with a weighting ? Yeah . And so then the {disfmarker} that {disfmarker} that makes the highly - matched the really big thing . Mm - hmm . Um , so , u i since they have these three categories , it seems like the reasonable thing to do {vocalsound} is to go across the languages {pause} and to come up with an improvement for each of those . Mm - hmm . Just say \" OK , in the {disfmarker} in the highly - matched case this is what happens , in the {disfmarker} {vocalsound} m the , uh {disfmarker} this other m medium if this happens , in the highly - mismatched {pause} that happens \" . Mm - hmm . And , uh , you should see , uh , a gentle degradation {pause} through that . Mmm . Um . But {disfmarker} I don't know . Yeah . I think that {disfmarker} that {disfmarker} I {disfmarker} I {disfmarker} I gather that in these meetings it 's {disfmarker} it 's really tricky to make anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has {disfmarker} has , uh , their own opinion Mm - hmm . and {disfmarker} I don't know . Yeah . Yeah . Uh , so {disfmarker} Yeah . Yeah , but there is probably a {disfmarker} a big change that will {vocalsound} be made is that the {disfmarker} the baseline {disfmarker} th they want to have a new baseline , perhaps , which is , um , MFCC but with {vocalsound} a voice activity detector . And apparently , {vocalsound} uh , some people are pushing to still keep this fifty percent number . So they want {vocalsound} to have at least fifty percent improvement on the baseline , but w which would be a much better baseline . Mm - hmm . Mm - hmm . And if we look at the result that Sunil sent , {vocalsound} just putting the VAD in the baseline improved , like , more than twenty percent , Mm - hmm . which would mean then {disfmarker} then {disfmarker} mean that fifty percent on this new baseline is like , well , more than sixty percent improvement on {disfmarker} on {disfmarker} o e e uh {disfmarker} So nobody would {pause} be there , probably . Right ? Right now , nobody would be there , but {disfmarker} Yeah . Good . Work to do . Uh - huh . So whose VAD is {disfmarker} Is {disfmarker} is this a {disfmarker} ? Uh , they didn't decide yet . I guess i this was one point of the conference call also , but {disfmarker} mmm , so I don't know . Um , but {disfmarker} Yeah . Oh . Oh , I {disfmarker} I think th that would be {vocalsound} good . I mean , it 's not that the design of the VAD isn't important , but it 's just that it {disfmarker} it {disfmarker} it does seem to be i uh , a lot of {pause} work to do a good job on {disfmarker} on that and as well as being a lot of work to do a good job on the feature {vocalsound} design , Yeah . so Yeah . if we can {pause} cut down on that maybe we can make some progress . M Yeah . Hmm . But I guess perhaps {disfmarker} I don't know w {vocalsound} Yeah . Uh , yeah . Per - e s s someone told that perhaps it 's not fair to do that because the , um {disfmarker} to make a good VAD {pause} you don't have enough to {disfmarker} with the {disfmarker} the features that are {disfmarker} the baseline features . So {disfmarker} mmm , you need more features . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the front - end . Yeah . So i Um , S sure . But i bu Wait a minute . I {disfmarker} I 'm confused . Yeah . Wha - what do you mean ? Yeah , if i So y so you m s Yeah , but {disfmarker} Well , let 's say for ins see , MFCC for instance doesn't have anything in it , uh , related to the pitch . So just {disfmarker} just for example . So suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it gets an unambiguous {disfmarker} Oh , oh . I see . Mm - hmm . if it gets an unambiguous result then you 're definitely in a {disfmarker} in a {disfmarker} in a voice in a , uh , s region with speech . Uh . So there 's this assumption that the v the voice activity detector can only use the MFCC ? That 's not clear , but this {disfmarker} {vocalsound} e Well , for the baseline . Yeah . So {disfmarker} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed to do better than ? I g Yeah . And so having the baseline be the MFCC 's {pause} means that people could {pause} choose to pour their ener their effort into trying to do a really good VAD I don't s But they seem like two {pause} separate issues . or tryi They 're sort of separate . Right ? I mean {disfmarker} Unfortunately there 's coupling between them , which is part of what I think Stephane is getting to , is that {vocalsound} you can choose your features in such a way as to improve the VAD . Yeah . And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may not be the same thing . But it seems like you should do both . You should do both Right ? and {disfmarker} and I {disfmarker} I think that this still makes {disfmarker} I still think this makes sense as a baseline . It 's just saying , as a baseline , we know {disfmarker} Mmm . you know , we had the MFCC 's before , lots of people have done voice activity detectors , Mm - hmm . you might as well pick some voice activity detector and make that the baseline , just like you picked some version of HTK and made that the baseline . Yeah . Right . And then {pause} let 's try and make everything better . Um , and if one of the ways you make it better is by having your features {pause} be better features for the VAD then that 's {disfmarker} so be it . Mm - hmm . But , uh , uh , uh , at least you have a starting point that 's {disfmarker} um , cuz i i some of {disfmarker} the some of the people didn't have a VAD at all , I guess . Right ? And {disfmarker} and Yeah . then they {disfmarker} they looked pretty bad and {disfmarker} and in fact what they were doing wasn't so bad at all . Mm - hmm . Mm - hmm . But , um . Yeah . It seems like you should try to make your baseline as good as possible . And if it turns out that {pause} you can't improve on that , well , I mean , then , you know , nobody wins and you just use MFCC . Right ? Yeah . I mean , it seems like , uh , it should include sort of the current state of the art {vocalsound} that you want {disfmarker} are trying to improve , and MFCC 's , you know , or PLP or something {disfmarker} it seems like {vocalsound} reasonable baseline for the features , and anybody doing this task , {vocalsound} uh , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um . It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker} Well , I think people just had You know ? it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue . Mmm . Mm - hmm . And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V A Ds and half of them didn't , and the half that did did well and the {vocalsound} half that didn't did poorly . Mm - hmm . So it 's {disfmarker} Mm - hmm . Um . Uh . Yeah . So we 'll see what happen with this . And {disfmarker} Yeah . So what happened since , um , {vocalsound} last week is {disfmarker} well , from OGI , these experiments on {pause} putting VAD on the baseline . And these experiments also are using , uh , some kind of noise compensation , so spectral subtraction , and putting on - line normalization , um , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction . Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones I {disfmarker} and {disfmarker} ? I have no idea , because the issue I brought up was with a very simple spectral subtraction approach , Mmm . and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz , Right . which was not done on our first proposal . When you say \" we have that \" , does Sunil have it now , too , I No . or {disfmarker} ? No . OK . Because we 're still testing . So we have the result for , {vocalsound} uh , just the features OK . and we are currently testing with putting the neural network in the KLT . Um , it seems to improve on the well - matched case , um , {vocalsound} but it 's a little bit worse on the mismatch and highly - mismatched {disfmarker} I mean when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . Mmm . But how much worse {disfmarker} since the weighting might change {disfmarker} how {disfmarker} how much worse is it on the other conditions , when you say it 's a little worse ? It 's like , uh , fff , fff {comment} {vocalsound} {pause} um , {comment} {vocalsound} {vocalsound} {pause} ten percent relative . Yeah . OK . Um . Mm - hmm . But it has the , uh {disfmarker} the latencies are much shorter . That 's {disfmarker} Uh - y w when I say it 's worse , it 's not {disfmarker} it 's when I {disfmarker} I {disfmarker} uh , compare proposal - two to proposal - one , so , r uh , y putting neural network {vocalsound} compared to n not having any neural network . I mean , this new system is {disfmarker} is {disfmarker} is better , Uh - huh . because it has {vocalsound} um , this sixty - four hertz cut - off , uh , clean {vocalsound} downsampling , and , um {disfmarker} what else ? Uh , yeah , a good VAD . We put the good VAD . So . Yeah , I don't know . I {disfmarker} I {disfmarker} j uh , uh {disfmarker} pr But the latencies {disfmarker} but you 've got the latency shorter now . Latency is short {disfmarker} is {disfmarker} Yeah . Yeah . Isn't it And so So it 's better than the system that we had before . Yeah . Mainly because {pause} {vocalsound} of {pause} the sixty - four hertz and the good VAD . OK . And then I took this system and , {vocalsound} mmm , w uh , I p we put the old filters also . So we have this good system , with good VAD , with the short filter and with the long filter , and , um , with the short filter it 's not worse . So {disfmarker} well , is it {disfmarker} OK . it 's in {disfmarker} So that 's {disfmarker} that 's all fine . Yes . Uh {disfmarker} But what you 're saying is that when you do these {disfmarker} So let me try to understand . When {disfmarker} when you do these same improvements {vocalsound} to proposal - one , Mm - hmm . that , uh , on the {disfmarker} i things are somewhat better , uh , in proposal - two for the well - matched case and somewhat worse for the other two cases . Yeah . So does , uh {disfmarker} when you say , uh {disfmarker} So {disfmarker} The th now that these other things are in there , is it the case maybe that the additions of proposal - two over proposal - one are {pause} less im important ? Yeah . Probably , yeah . I get it . Um {disfmarker} So , yeah . Uh . Yeah , but it 's a good thing anyway to have {vocalsound} shorter delay . Then we tried , um , {vocalsound} to do something like proposal - two but having , um , e using also MSG features . So there is this KLT part , which use just the standard features , Mm - hmm . Right . and then two neura two neural networks . Mm - hmm . Mmm , and it doesn't seem to help . Um , however , we just have {vocalsound} one result , which is the Italian mismatch , so . Uh . We have to wait for that to fill the whole table , but {disfmarker} OK . There was a {vocalsound} start of some effort on something related to voicing or something . Is that {disfmarker} ? Yeah . Um , {vocalsound} yeah . So basically we try to , {vocalsound} {vocalsound} uh , find {vocalsound} good features that could be used for voicing detection , uh , but it 's still , uh {disfmarker} on the , um {disfmarker} t Oh , well , I have the picture . we {disfmarker} w basically we are still playing with Matlab to {disfmarker} {vocalsound} to look at {disfmarker} at what happened , What sorts of {disfmarker} Yeah . and {disfmarker} what sorts of features are you looking at ? We have some {disfmarker} So we would be looking at , um , the {pause} variance of the spectrum of the excitation , uh , um , this , this , and this . something like this , which is {disfmarker} should be high for voiced sounds . Uh , we {disfmarker} Wait a minute . I {disfmarker} what does that mean ? The variance of the spectrum of excitation . Yeah . So the {disfmarker} So basically the spectrum of the excitation {vocalsound} for a purely periodic sig signal shou sh OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel {disfmarker} mel {disfmarker} {vocalsound} mel filter , uh , spectrum from the FFT spectrum . e That 's right . Yeah . So {disfmarker} Right . Yeah . Mm - hmm . So we have the mel f filter bank , we have the FFT , so we {pause} just {disfmarker} So it 's {disfmarker} it 's not really an excitation , No . but it 's something that hopefully tells you something about the excitation . Yeah , that 's right . Yeah , yeah . Um {disfmarker} Yeah . We have here some histogram , E yeah , but they have a lot of overlap . but it 's {disfmarker} it 's still {disfmarker} Yeah . So , well , for unvoiced portion we have something tha {vocalsound} that has a mean around O point three , and for voiced portion the mean is O point fifty - nine . But the variance seem quite {vocalsound} high . How do you know {disfmarker} ? So {disfmarker} Mmm . How did you get your {pause} voiced and unvoiced truth data ? We used , uh , TIMIT and we used canonical mappings between the phones Yeah . We , uh , use {pause} TIMIT on this , and for {disfmarker} th Yeah . But if we look at it in one sentence , it {disfmarker} apparently it 's good , I think . Yeah , but {disfmarker} Yeah . Uh , so it 's noisy TIMIT . That 's right . Yeah . It 's noisy TIMIT . Yeah . It seems quite robust to noise , so when we take {disfmarker} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close . Mm - hmm . Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker} Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ? Right now we just are trying to find some features . And , Mm - hmm . uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker} Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging . Mm - hmm . Well , yeah , except the variance was big . Right ? Yeah . Except the variance is quite high . Right ? Well , y Yeah . Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings . Uh - huh . Right ? So , really that 's sort of a cartoon picture about what 's voiced and unvoiced . So that could be giving you a lot of variance . Yeah . I mean , i it {disfmarker} it may be that {disfmarker} that you 're finding something good and that the variance is sort of artificial because of how you 're getting your truth . Mm - hmm . Yeah . But another way of looking at it {vocalsound} might be that {disfmarker} I mean , what w we we are coming up with feature sets after all . So another way of looking at it is that {vocalsound} um , the mel cepstru mel {pause} spectrum , mel cepstrum , {vocalsound} any of these variants , um , give you the smooth spectrum . It 's the spectral envelope . By going back to the FFT , {vocalsound} you 're getting something that is {pause} more like the raw data . So the question is , what characterization {disfmarker} and you 're playing around with this {disfmarker} another way of looking at it is what characterization {vocalsound} of the difference between {pause} the raw data {pause} and this smooth version {pause} is something that you 're missing that could help ? So , I mean , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best {disfmarker} the world 's best voiced - unvoiced , uh , uh , classifier , Mm - hmm . Mmm . but it 's more that , {vocalsound} you know , uh , uh , try some different statistical characterizations of that difference back to the raw data Right . and {disfmarker} and m maybe there 's something there that {pause} the system can use . Right . Yeah . Yeah , but ther more obvious is that {disfmarker} Yeah . The {disfmarker} the more obvious is that {disfmarker} that {disfmarker} well , using the {disfmarker} th the FFT , um , {vocalsound} you just {disfmarker} it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So , Yeah . this is why we {disfmarker} we started to look {pause} by having sort of voiced phonemes Well , that 's the rea w w what I 'm arguing is that 's Yeah . I mean , uh , what I 'm arguing is that that {disfmarker} that 's givi you {disfmarker} gives you your intuition . and {disfmarker} Mm - hmm . But in {disfmarker} in reality , it 's {disfmarker} you know , there 's all of this {disfmarker} this overlap and so forth , Oh , sorry . and {disfmarker} But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and {disfmarker} and then , uh , th you know , structural reasons , uh , uh , like the one that Chuck said , that {disfmarker} that in fact , well , the data itself is {disfmarker} {vocalsound} that you 're working with is not perfect . Yeah . Mm - hmm . So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly , Mm - hmm . but it 's just some characterization {vocalsound} of something back in the {disfmarker} in the {disfmarker} in the almost raw data , rather than the smooth version . Mm - hmm . And your intuition is driving you towards particular kinds of , {vocalsound} uh , statistical characterizations of , um , what 's missing from the spectral envelope . Mm - hmm . Um , obviously you have something about the excitation , um , and what is it about the excitation , and , you know {disfmarker} and you 're not getting the excitation anyway , you know . So {disfmarker} so I {disfmarker} I would almost take a {disfmarker} uh , especially if {disfmarker} if these trainings and so forth are faster , I would almost just take a {vocalsound} uh , a scattershot at a few different {vocalsound} ways of look of characterizing that difference and , uh , you could have one of them but {disfmarker} and {disfmarker} and see , you know , which of them helps . Mm - hmm . OK . So i is the idea that you 're going to take {pause} whatever features you develop and {disfmarker} and just add them onto the future vector ? Or , what 's the use of the {disfmarker} the voiced - unvoiced detector ? Uh , I guess we don't know exactly yet . But , {vocalsound} um {disfmarker} Yeah . Th It 's not part of a VAD system that you 're doing ? No . Uh , no . No . Oh , OK . No , the idea was , I guess , to {disfmarker} to use them as {disfmarker} as features . Features . I see . Uh {disfmarker} Yeah , it could be , uh {disfmarker} it could be {vocalsound} a neural network that does voiced and unvoiced detection , Mm - hmm . but it could be in the {disfmarker} also the big neural network that does phoneme classification . Mm - hmm . Mmm . Yeah . But each one of the mixture components {disfmarker} I mean , you have , uh , uh , variance only , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it 's {disfmarker} so , uh , it seems l you know {disfmarker} I think it 's a neat thing . Uh , it seems like a good idea . Yeah . Um . Yeah . I mean , {vocalsound} I know that , um , people doing some robustness things a ways back were {disfmarker} were just doing {disfmarker} just being gross and just throwing in the FFT and actually it wasn't {disfmarker} wasn't {disfmarker} wasn't so bad . Uh , so it would s and {disfmarker} and you know that i it 's gotta hurt you a little bit to not have a {disfmarker} {vocalsound} a spectral , uh {disfmarker} a s a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker} Mm - hmm . that , uh {disfmarker} uh {disfmarker} So . So how does {disfmarker} uh , maybe I 'm going in too much detail , but {vocalsound} how exactly do you make the difference between the FFT and the smoothed {pause} spectral envelope ? Wha - wh i i uh , how is that , uh {disfmarker} ? Um , we just {disfmarker} How did we do it up again ? Uh , we distend the {disfmarker} we have the twenty - three coefficient af after the mel f {vocalsound} filter , Mm - hmm . and we extend these coefficient between the {disfmarker} all the frequency range . Mm - hmm . And i the interpolation i between the point {vocalsound} is {disfmarker} give for the triang triangular filter , the value of the triangular filter and of this way we obtained this mode this model speech . S So you essentially take the values that {disfmarker} th that you get from the triangular filter and extend them to sor sort of like a rectangle , that 's at that m value . Yeah . Yeah . I think we have linear interpolation . Mm - hmm . So we have {disfmarker} we have one point for {disfmarker} one energy for each filter bank , mmm Yeah , it 's linear . Mmm . Oh . which is {pause} the energy {pause} that 's centered on {disfmarker} on {disfmarker} on the triangle {disfmarker} Yeah . At the n at the center of the filter {disfmarker} So you {disfmarker} you end up with a vector that 's the same length as the FFT {pause} vector ? Yeah . That 's right . Yeah . And then you just , uh , compute differences Yeah . I have here one example if you {disfmarker} if you want see something like that . Then we compute the difference . and , Yeah . Uh - huh . OK . uh , sum the differences ? So . And I think the variance is computed only from , like , two hundred hertz to {pause} one {disfmarker} to fifteen hundred . Oh ! OK . Mm - hmm . Two thou two {disfmarker} {comment} fifteen hundred ? Mm - hmm . Because {disfmarker} No . Right . Two hundred and fifty thousand . Fifteen hundred . Because {disfmarker} Yeah . Yeah . Two thousand and fifteen hundred . Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker} Yeah . Well , it 's just {disfmarker} No , it 's {disfmarker} makes sense to look at {pause} low frequencies . So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ? Yeah . Right . So i so i i this is {disfmarker} I mean , i you could argue about whether it should be linear interpolation or {disfmarker} or {disfmarker} or {disfmarker} or zeroeth order , but {disfmarker} but Uh - huh . at any rate something like this {pause} is what you 're feeding your recognizer , typically . Like which of the {disfmarker} ? No . Uh , so the mel cepstrum is the {disfmarker} is the {disfmarker} is the cepstrum of this {disfmarker} {vocalsound} this , uh , spectrum or log spectrum , So this is {disfmarker} Yeah . Yeah . Right , right . whatever it {disfmarker} You - you 're subtracting in {disfmarker} in {disfmarker} in {vocalsound} power domain or log domain ? In log domain . Yeah . Log domain . OK . So it 's sort of like division , when you do the {disfmarker} yeah , the spectra . Yeah . Uh , yeah . It 's the ratio . Um . Yeah . But , anyway , um {disfmarker} and that 's {disfmarker} So what 's th uh , what 's the intuition behind this kind of a thing ? I {disfmarker} I don't know really know the signal - processing well enough to understand what {disfmarker} {vocalsound} what is that doing . So . Yeah . What happen if {disfmarker} what we have {disfmarker} have {disfmarker} what we would like to have is {pause} some spectrum of the excitation signal , Yeah . I guess that makes sense . Yeah . which is for voiced sound ideally a {disfmarker} a pulse train Uh - huh . and for unvoiced it 's something that 's more flat . Uh - huh . Right . And the way to do this {vocalsound} is that {disfmarker} well , we have the {disfmarker} we have the FFT because it 's computed in {disfmarker} in the {disfmarker} in the system , and we have {vocalsound} the mel {vocalsound} filter banks , Mm - hmm . Mm - hmm . and so if we {disfmarker} if we , like , remove the mel filter bank from the FFT , {vocalsound} we have something that 's {pause} close to the {pause} excitation signal . Oh . It 's something that 's like {vocalsound} a {disfmarker} a a train of p a pulse train for voiced sound OK . Yeah . Oh ! OK . Yeah . and that 's {disfmarker} that should be flat for {disfmarker} Yeah . I see . So do you have a picture that sh ? So - It 's {disfmarker} Y Is this for a voiced segment , yeah . this picture ? What does it look like for unvoiced ? Yeah . You have several {disfmarker} some unvoiced ? The dif No . Unvoiced , I don't have Oh . for unvoiced . Yeah . So , you know , all {disfmarker} I 'm sorry . But {disfmarker} Yeah . Yeah . Yeah . This is the {disfmarker} between {disfmarker} This is another voiced example . Yeah . No . But it 's this , Oh , yeah . This is {disfmarker} but between the frequency that we are considered for the excitation {disfmarker} Right . Mm - hmm . for the difference and this is the difference . Yeah . This is the difference . OK . So , of course , it 's around zero , Yeah . Sure looks {disfmarker} but {disfmarker} Hmm . Well , no . Hmm . It is {disfmarker} Yeah . Because we begin , {vocalsound} uh , in fifteen {vocalsound} point {disfmarker} the fifteen point . So , does {disfmarker} does the periodicity of this signal say something about the {disfmarker} the {disfmarker} Fifteen p So it 's {disfmarker} Yeah . Pitch . It 's the pitch . the pitch ? Yeah . Mm - hmm . Yeah . OK . That 's like fundamental frequency . Mm - hmm . So , I mean , i t t OK . I see . I mean , to first order {vocalsound} what you 'd {disfmarker} what you 're doing {disfmarker} I mean , ignore all the details and all the ways which is {disfmarker} that these are complete lies . Uh , the {disfmarker} the {disfmarker} you know , what you 're doing in feature extraction for speech recognition is you have , {vocalsound} uh , in your head a {disfmarker} a {disfmarker} a {disfmarker} a simplified production model for speech , Mm - hmm . in which you have a periodic or aperiodic source that 's driving some filters . Mm - hmm . Yeah . This is the {disfmarker} the auto - correlation {disfmarker} the R - zero energy . Do you have the mean {disfmarker} do you have the mean for the auto - correlation {disfmarker} ? Uh , first order for speech recognition , you say \" I don't care about the source \" . For {disfmarker} Yeah . Well , I mean for the {disfmarker} the energy . I have the mean . Right ? Right . And so you just want to find out what the filters are . Right . Yeah . The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation . Here . They should be more close . Ah , no . This is this ? More close . Is this ? And this . Mm - hmm . Yeah . Mm - hmm . So they are {disfmarker} this is {disfmarker} there is less difference . Mm - hmm . So if you look at the spectral envelope , just the very smooth properties of it , {vocalsound} you get something closer to that . This is less {disfmarker} it 's less robust . Less robust . Yeah . Oh , yeah . And the notion is if you have the full spectrum , with all the little nitty - gritty details , {vocalsound} that that has the effect of both , Yeah . and it would be a multiplication in {disfmarker} in frequency domain Mm - hmm . so that would be like an addition in log {disfmarker} {vocalsound} power spectrum domain . Mm - hmm . Mm - hmm . And so this is saying , well , if you really do have that {vocalsound} sort of vocal tract envelope , and you subtract that off , what you get is the excitation . And I call that lies because you don't really have that , you just have some kind of {vocalsound} signal - processing trickery to get something that 's kind of smooth . It 's not really what 's happening in the vocal tract Yeah . so you 're not really getting the vocal excitation . Right . That 's why I was going to the {disfmarker} why I was referring to it in a more {disfmarker} {vocalsound} a more , uh , {vocalsound} uh , {vocalsound} conservative way , when I was saying \" well , it 's {disfmarker} yeah , it 's the excitation \" . But it 's not really the excitation . It 's whatever it is that 's different between {disfmarker} Oh . This moved in the {disfmarker} So {disfmarker} so , stand standing back from that , you sort of say there 's this very detailed representation . Yeah . You go to a smooth representation . Mm - hmm . You go to a smooth representation cuz this typically generalizes better . Mm - hmm . Um , but whenever you smooth you lose something , so the question is have you lost something you can you use ? Right . Um , probably you wouldn't want to go to the extreme of just ta saying \" OK , our feature set will be the FFT \" , cuz we really think we do gain something in robustness from going to something smoother , but maybe there 's something that we missed . Mm - hmm . So what is it ? Yeah . And then you go back to the intuition that , well , you don't really get the excitation , but you get something related to it . Mm - hmm . And it {disfmarker} and as you can see from those pictures , you do get something {vocalsound} that shows some periodicity , uh , in frequency , Mm - hmm . you know , and {disfmarker} and {disfmarker} and also in time . Hmm . So {disfmarker} That 's {disfmarker} that 's really neat . so , So you don't have one for unvoiced {pause} picture ? Uh , not here . Oh . No , I have s Mm - hmm . Yeah . But not here . But presumably you 'll see something that won't have this kind of , uh , uh , uh , regularity in frequency , uh , in the {disfmarker} But {disfmarker} Yeah . Well . Not here . I would li I would like to see those {pause} pictures . Well , so . Yeah . I can't see you {comment} now . Yeah . Yeah . Yeah . Mm - hmm . I don't have . And so you said this is pretty {disfmarker} doing this kind of thing is pretty robust to noise ? It seems , yeah . Um , Huh . Pfft . Oops . The mean is different {vocalsound} with it , because the {disfmarker} {vocalsound} the histogram for the {disfmarker} {vocalsound} the classifica No , no , no . But th the kind of robustness to noise {disfmarker} Oh ! So if {disfmarker} if you take this frame , {vocalsound} uh , from the noisy utterance and the same frame from the clean utterance {disfmarker} Hmm . You end up with a similar difference Y y y yeah . We end up with {disfmarker} over here ? Yeah . OK . Cool ! I have here the same frame for the {pause} clean speech {disfmarker} Oh , that 's clean . the same cle Oh , OK But they are a difference . Yeah , that 's {disfmarker} Because here the FFT is only with {vocalsound} two hundred fifty - six point Oh . and this is with five hundred {pause} twelve . Yeah . This is kind of inter interesting also OK . because if we use the standard , {vocalsound} uh , frame length of {disfmarker} of , like , twenty - five milliseconds , {vocalsound} um , {vocalsound} what happens is that for low - pitched voiced , because of the frame length , y you don't really have {disfmarker} {vocalsound} you don't clearly see this periodic structure , Mm - hmm . because of the first lobe of {disfmarker} of each {disfmarker} each of the harmonics . So this one inclu is a longer {disfmarker} Ah . So , this is like {disfmarker} yeah , fifty milliseconds or something like that . Fifty millis Yeah . Yeah , but it 's the same frame and {disfmarker} Oh , it 's that time - frequency trade - off thing . Yeah . Right ? I see . Yeah . So , yeah . Mm - hmm . Oh . Oh , so this i is this the difference here , for that ? No . This is the signal . This is the signal . I see that . Oh , yeah . The frame . Oh , that 's the f the original . Yeah . This is the fra the original frame . So with a short frame basically you have only two periods Yeah . and it 's not {disfmarker} not enough to {disfmarker} to have this kind of neat things . Mm - hmm . Mm - hmm . Yeah . But {disfmarker} And here {disfmarker} No , well . Yeah . So probably we 'll have to use , {vocalsound} like , long f long frames . Mm - hmm . Mm - hmm . Hmm . Oh . Mmm . That 's interesting . Yeah , maybe . Well , I mean it looks better , but , I mean , the thing is if {disfmarker} if , uh {disfmarker} if you 're actually asking {disfmarker} you know , if you actually j uh , need to do {disfmarker} place along an FFT , it may be {disfmarker} it may be pushing things . Yeah . And {disfmarker} and , uh {disfmarker} Would you {disfmarker} would you wanna do this kind of , uh , difference thing {vocalsound} after you do spectral subtraction ? Uh , {vocalsound} maybe . No . Maybe we can do that . Mmm . Hmm . The spectral subtraction is being done at what level ? Is it being done at the level of FFT bins or at the level of , uh , mel spectrum or something ? Um , I guess it depends . I mean , how are they doing it ? How they 're doing it ? Yeah . Um , I guess Ericsson is on the , um , filter bank , FFT . Filter bank , no ? It 's on the filter bank , yeah . so . So , yeah , probably {disfmarker} I i it {disfmarker} Yeah . So in that case , it might not make much difference at all . Seems like you 'd wanna do it on the FFT bins . Maybe . I mean , certainly it 'd be better . I I mean , if you were gonna {disfmarker} uh , for {disfmarker} for this purpose , that is . Mm - hmm . Yeah . Mm - hmm . Yeah . OK . Mmm . What else ? Uh . {vocalsound} Yeah , that 's all . So we 'll perhaps {vocalsound} {vocalsound} {vocalsound} try to convince OGI people to use the new {disfmarker} {vocalsound} the new filters and {disfmarker} Yeah . OK . Uh , has {disfmarker} has anything happened yet on this business of having some sort of standard , uh , source , Uh , not yet or {disfmarker} ? but I wi I will {vocalsound} call them and {disfmarker} OK . now they are {disfmarker} I think they have more time because they have this {disfmarker} well , Eurospeech deadline is {vocalsound} over When is the next , um , Aurora {pause} deadline ? and {disfmarker} It 's , um , in June . Yeah . June . Early June , late June , middle June ? I don't know w Hmm . Hmm . OK . Um , and {pause} he 's been doing all the talking but {disfmarker} but {vocalsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disfmarker} Yeah . This is {disfmarker} this by the way a bad thing . We 're trying to get , um , m more female voices in this record as well . So . Make sur make sure Carmen {vocalsound} talks as well . Uh , but has he pretty much been talking about what you 're doing also , and {disfmarker} ? Oh , I {disfmarker} I am doing this . Yes . Yeah , yeah . I don't know . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak . Yeah , well . Because {disfmarker} You know , uh , we 'll get {disfmarker} we 'll get to , uh , Spanish voices sometime , and {vocalsound} we do {disfmarker} we want to recognize , {vocalsound} uh , you too . After the {disfmarker} after , uh , the result for the TI - digits {vocalsound} on the meeting record there will be foreigns people . Yeah , but {disfmarker} Oh , no . Y We like {disfmarker} we {disfmarker} we 're {disfmarker} we 're {disfmarker} w we are {disfmarker} we 're in the , uh , Bourlard - Hermansky - Morgan , uh , frame of mind . Yeah , we like high error rates . It 's {disfmarker} Yeah . That way there 's lots of work to do . So it 's {disfmarker} Uh , anything to talk about ? N um , not not not much is new . So when I talked about what I 'm planning to do last time , {vocalsound} I said I was , um , going to use Avendano 's method of , um , {vocalsound} using a transformation , um , {vocalsound} to map from long analysis frames which are used for removing reverberation to short analysis frames for feature calculation . He has a trick for doing that {pause} involving viewing the DFT as a matrix . Um , but , uh , um , I decided {vocalsound} not to do that after all because I {disfmarker} I realized to use it I 'd need to have these short analysis frames get plugged directly into the feature computation somehow Mm - hmm . and right now I think our feature computation is set to up to , um , {vocalsound} take , um , audio as input , in general . So I decided that I {disfmarker} I 'll do the reverberation removal on the long analysis windows and then just re - synthesize audio and then send that . This is in order to use the SRI system or something . Right ? Um , or {disfmarker} or even if I 'm using our system , I was thinking it might be easier to just re - synthesize the audio , Yeah ? because then I could just feacalc as is and I wouldn't have to change the code . Oh , OK . Yeah . I mean , it 's {disfmarker} um , certainly in a short {disfmarker} short - term this just sounds easier . Uh - huh . Yeah . I mean , longer - term if it 's {disfmarker} {vocalsound} if it turns out to be useful , one {disfmarker} one might want to do something else , Right . That 's true . but {disfmarker} Uh , uh , I mean , in {disfmarker} in other words , you {disfmarker} you may be putting other kinds of errors in {pause} from the re - synthesis process . But {disfmarker} e u From the re - synthesis ? Um , Yeah . O - OK . I don't know anything about re - synthesis . Uh , how likely do you think that is ? Uh , it depends what you {disfmarker} what you do . I mean , it 's {disfmarker} it 's {disfmarker} it 's , uh , um {disfmarker} Don't know . But anyway it sounds like a reasonable way to go for a {disfmarker} for an initial thing , and we can look at {disfmarker} {vocalsound} at exactly what you end up doing and {disfmarker} and then figure out if there 's some {disfmarker} {vocalsound} something that could be {disfmarker} be hurt by the end part of the process . OK . OK . So that 's {disfmarker} That was it , huh ? That {disfmarker} Yeah , e That 's it , that 's it . OK . OK . Uh - huh . Um , anything to {pause} add ? Um . Well , I 've been continuing reading . I went off on a little tangent this past week , um , looking at , uh , {vocalsound} uh , modulation s spectrum stuff , um , and {disfmarker} and learning a bit about what {disfmarker} what , um {disfmarker} what it is , and , uh , the importance of it in speech recognition . And I found some {disfmarker} {vocalsound} some , uh , neat papers , {vocalsound} um , historical papers from , {vocalsound} um , {vocalsound} Kanedera , Hermansky , and Arai . Yeah . And they {disfmarker} they did a lot of experiments where th where , {vocalsound} um , they take speech {vocalsound} and , um , e they modify {vocalsound} the , uh {disfmarker} they {disfmarker} they {disfmarker} they measure the relative importance of having different , um , portions of the modulation spectrum intact . Yeah . And they find that the {disfmarker} the spectrum between one and sixteen hertz in the modulation {vocalsound} is , uh {disfmarker} is im important for speech recognition . Sure . I mean , this sort of goes back to earlier stuff by Drullman . Um . And {disfmarker} and , uh , the {disfmarker} the MSG features were sort of built up {vocalsound} with this notion {disfmarker} Yeah . Right . But , I guess , I thought you had brought this up in the context of , um , targets somehow . Right . But i m Um {disfmarker} i it 's not {disfmarker} I mean , they 're sort of not in the same kind of category as , say , a phonetic target or a syllabic target Mmm . Mm - hmm . or a {disfmarker} Um , I was thinking more like using them as {disfmarker} as the inputs to {disfmarker} to the detectors . or a feature or something . Oh , I see . Well , that 's sort of what MSG does . Yeah . Yeah . Right ? So it 's {disfmarker} Mm - hmm . But {disfmarker} but , uh {disfmarker} S Yeah . Yeah . Anyway , we 'll talk more about it later . OK . Yeah . We can talk more about it later . Yeah . Yeah . Yeah . So maybe , {vocalsound} le Should we do digits ? let 's do digits . Let you {disfmarker} you start . Oh , OK . L fifty . Right .",
        "summarize": null
    }
]
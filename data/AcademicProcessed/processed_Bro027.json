[
    {
        "transcript": "OK , we 're going . Eight , eight ? This is three . Three . Yep . Yep . Test . Hmm . Let 's see . Move it bit . Test ? Test ? OK , I guess it 's alright . So , let 's see . Yeah , Barry 's not here and Dave 's not here . Um , I can say about {disfmarker} just q just quickly to get through it , that Dave and I submitted this ASRU . This is for ASRU . Yeah . So . Um . Yeah , it 's {disfmarker} it 's interesting . I mean , basically we 're dealing with rever reverberation , and , um , when we deal with pure reverberation , the technique he 's using works really , really well . Uh , and when they had the reverberation here , uh , we 'll measure the signal - to - noise ratio and it 's , uh , about nine DB . So , Hmm . um , You mean , from the actual , uh , recordings ? a fair amount of {disfmarker} k It 's nine DB ? Yeah . Yeah . Um {disfmarker} And actually it brought up a question which may be relevant to the Aurora stuff too . Um , I know that when you figured out the filters that we 're using for the Mel scale , there was some experimentation that went on at {disfmarker} at , uh {disfmarker} at OGI . Um , but one of the differences that we found between the two systems that we were using , {comment} the {disfmarker} the Aurora HTK system baseline system {comment} and the system that we were {disfmarker} the {disfmarker} the uh , other system we were using , the uh , the SRI system , was that the SRI system had maybe a , um , hundred hertz high - pass . And the , uh , Aurora HTK , it was like twenty . Yep . S sixty - four . Uh . S sixty - four . Sixty - four ? Uh . Yeah , if you 're using the baseline . Is that the ba band center ? No , the edge . The edge is really , uh , sixty - four ? Yeah . For some reason , uh , Dave thought it was twenty ,  So the , uh , center would be somewhere around like hundred but . and {disfmarker} hundred and {disfmarker} hundred {disfmarker} hundred and {disfmarker} maybe {disfmarker} it 's like {disfmarker} fi hundred hertz . But do you know , for instance , h how far down it would be at twenty hertz ? What the {disfmarker} how much rejection would there be at twenty hertz , let 's say ? At twenty hertz . Yeah , any idea what the curve looks like ? Twenty hertz frequency {disfmarker} Oh , it 's {disfmarker} it 's zero at twenty hertz , right ? The filter ? Yea - actually , the left edge of the first filter is at sixty - four . Sixt - s sixty - four . So {disfmarker} So anything less than sixty - four is zero . Mmm . It 's actually set to zero ? What kind of filter is that ? Yeah . Yeah . Is this {disfmarker} oh , from the {disfmarker} from {disfmarker} It {disfmarker} This is the filter bank in the frequency domain that starts at sixty - four . Oh , so you , uh {disfmarker} so you really set it to zero , the FFT ? Yeah , Yeah . yeah . So it 's {disfmarker} it 's a weight on the ball spectrum . Triangular weighting . Right . OK . Um {disfmarker} OK . So that 's {disfmarker} that 's a little different than Dave thought , I think . But {disfmarker} but , um , still , it 's possible that we 're getting in some more noise . So I wonder , is it {disfmarker} @ @ Was there {disfmarker} their experimentation with , uh , say , throwing away that filter or something ? And , uh {disfmarker} Uh , throwing away the first ? Yeah . Um , yeah , we {disfmarker} we 've tried including the full {disfmarker} full bank . Right ? From zero to four K . Mm - hmm . And that 's always worse than using sixty - four hertz . Right , but the question is , whether sixty - four hertz is {disfmarker} is , uh , too , uh , low . Yeah , I mean , make it a hundred or so ? Yeah . I t I think I 've tried a hundred and it was more or less the same , or slightly worse . On what test set ? On the same , uh , SpeechDat - Car , Aurora . Um , it was on the SpeechDat - Car . Yeah . So I tried a hundred to four K . Yeah . Um , So it was {disfmarker} and on {disfmarker} and on the , um , um , {vocalsound} TI - digits also ? No , no , no . I think I just tried it on SpeechDat - Car . Mmm . That 'd be something to look at sometime because what , um , eh , he was looking at was performance in this room . Mm - hmm . Would that be more like {disfmarker} Well , you 'd think that 'd be more like SpeechDat - Car , I guess , in terms of the noise . The SpeechDat - Car is more , uh , sort of roughly stationary , a lot of it . And {disfmarker} and TI - digits maybe is not so much as {disfmarker} Yeah . Mm - hmm . Yeah . Yeah . Mm - hmm . OK . Well , maybe it 's not a big deal . But , um {disfmarker} Anyway , that was just something we wondered about . But , um , uh , certainly a lot of the noise , uh , is , uh , below a hundred hertz . Uh , the signal - to - noise ratio , you know , looks a fair amount better if you {disfmarker} if you high - pass filter it from this room . Yeah . But , um {disfmarker} but it 's still pretty noisy . Even {disfmarker} even for a hundred hertz up , it 's {disfmarker} it 's still fairly noisy . The signal - to - noise ratio is {disfmarker} is {disfmarker} is actually still pretty bad . Mm - hmm . Hmm . So , um , I mean , the main {disfmarker} the {disfmarker} the {disfmarker} So that 's on th that 's on the f the far field ones though , right ? Yeah . Yeah , that 's on the far field . Yeah , the near field 's pretty good . So wha what is , uh {disfmarker} what 's causing that ? Well , we got a {disfmarker} a video projector in here , uh , and , uh {disfmarker} which we keep on during every {disfmarker} every session we record , Yeah . which , you know , I {disfmarker} I {disfmarker} w we were aware of Uh - huh . but {disfmarker} but we thought it wasn't a bad thing . Yeah . I mean , that 's a nice noise source . Uh , and there 's also the , uh {disfmarker} uh , air conditioning . Hmm . Which , uh , you know , is a pretty low frequency kind of thing . Mm - hmm . But {disfmarker} but , uh {disfmarker} So , those are {disfmarker} those are major components , I think , I see . uh , for the stationary kind of stuff . Mmm . Um , but , um , it , uh {disfmarker} I guess , I {disfmarker} maybe I said this last week too but it {disfmarker} it {disfmarker} it really became apparent to us that we need to {disfmarker} to take account of noise . And , uh , so I think when {disfmarker} when he gets done with his prelim study I think {vocalsound} one of the next things we 'd want to do is to take this , uh {disfmarker} uh , noise , uh , processing stuff and {disfmarker} and , uh {disfmarker} uh , synthesize some speech from it . When are his prelims ? And then {disfmarker} Um , I think in about , um , a little less than two weeks . Oh . Wow . Yeah . Yeah . So . Uh , it might even be sooner . Uh , let 's see , this is the sixteenth , seventeenth ? Yeah , I don't know if he 's before {disfmarker} It might even be in a week . So , I A week , Huh . I {disfmarker} I guessed that they were gonna do it some time during the semester week and a half . but they 'll do it any time , huh ? They seem to be {disfmarker} Well , the semester actually is starting up . Is it already ? Yeah , the semester 's late {disfmarker} late August they start here . Yikes . So they do it right at the beginning of the semester . Yeah . Yeah . So , uh {disfmarker} Yep . I mean , that {disfmarker} that was sort of one {disfmarker} I mean , the overall results seemed to be first place in {disfmarker} in {disfmarker} in the case of either , um , artificial reverberation or a modest sized training set . Uh , either way , uh , i uh , it helped a lot . And {disfmarker} But if you had a {disfmarker} a really big training set , a recognizer , uh , system that was capable of taking advantage of a really large training set {disfmarker} I thought that {disfmarker} One thing with the HTK is that is has the {disfmarker} as we 're using {disfmarker} the configuration we 're using is w s is {disfmarker} being bound by the terms of Aurora , we have all those parameters just set as they are . So even if we had a hundred times as much data , we wouldn't go out to , you know , ten or t or a hundred times as many Gaussians or anything . So , um , it 's kind of hard to take advantage of {disfmarker} of {disfmarker} of big chunks of data . Uh , whereas the other one does sort of expand as you have more training data . Mm - hmm . Mmm , yeah . It does it automatically , actually . And so , um , uh , that one really benefited from the larger set . And it was also a diverse set with different noises and so forth . Uh , so , um , that , uh {disfmarker} that seemed to be {disfmarker} So , if you have that {disfmarker} that better recognizer that can {disfmarker} that can build up more parameters , and if you , um , have the natural room , which in this case has a p a pretty bad signal - to - noise ratio , then in that case , um , the right thing to do is just do {disfmarker} u use speaker adaptation . And {disfmarker} and not bother with {disfmarker} with this acoustic , uh , processing . But I think that that would not be true if we did some explicit noise - processing as well as , uh , the convolutional kind of things we were doing . Mm - hmm . So . That 's sort of what we found . Hmm . I , um {disfmarker} {vocalsound} uh , started working on the uh {disfmarker} Mississippi State recognizer . So , I got in touch with Joe and {disfmarker} and , uh , from your email and things like that . Oh , OK . And , uh , they added me to the list {disfmarker} uh , the mailing list . OK , great . And he gave me all of the pointers and everything that I needed . And so I downloaded the , um {disfmarker} There were two things , uh , that they had to download . One was the , uh , I guess the software . And another wad {disfmarker} was a , um , sort of like a sample {disfmarker} a sample run . So I downloaded the software and compiled all of that . And it compiled fine . Eight . No problems . Oh , eh , great . And , um , I grabbed the sample stuff but I haven't , uh , compiled it . That sample was released only yesterday or the day before , right ? No {disfmarker} Well , I haven't grabbed that one yet . So there 's two . Oh , there is another short sample set {disfmarker} There was another short one , yeah . o o sample . And so I haven't grabbed the latest one that he just , uh , put out yet . OK . Oh , OK . F Yeah , OK . So . Um , but , the software seemed to compile fine and everything , so . And , um , So . Is there any word yet about the issues about , um , adjustments for different feature sets or anything ? No , I {disfmarker} I d You asked me to write to him and I think I forgot to ask him about that . Or if I did ask him , he didn't reply . Yeah . I {disfmarker} I don't remember yet . Uh , I 'll {disfmarker} I 'll d I 'll double check that and ask him again . Yeah . Yeah , it 's like that {disfmarker} that could r turn out to be an important issue for us . Hmm . Mmm . Yeah . Yeah . Yeah . Cuz they have it {disfmarker} Maybe I 'll send it to the list . Yeah . Cuz they have , uh , already frozen those in i insertion penalties and all those stuff is what {disfmarker} I feel . Because they have this document explaining the recognizer . Uh - huh . And they have these tables with , uh , various language model weights , insertion penalties . OK , I haven't seen that one yet . u So . Uh , it 's th it 's there on that web . OK . And , uh , on that , I mean , they have run some experiments using various insertion penalties and all those {disfmarker} And so they 've picked {disfmarker} the values . Yeah , I think they pi p Oh , OK . yeah , they picked the values from {disfmarker} OK . For r w what test set ? Uh , p the one that they have reported is a NIST evaluation , Wall Street Journal . But that has nothing to do with what we 're testing on , right ? Mm - hmm . You know . No . So they 're , like {disfmarker} um {disfmarker} So they are actually trying to , uh , fix that {disfmarker} those values using the clean , uh , training part of the Wall Street Journal . Which is {disfmarker} I mean , the Aurora . Aurora has a clean subset . Right . I mean , they want to train it and then this {disfmarker} they 're going to run some evaluations . So they 're set they 're setting it based on that ? Yeah . OK . So now , we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters . Yeah . But , um , Yeah . uh {disfmarker} but it 's still worth , I think , just {disfmarker} since {disfmarker} you know , just chatting with Joe about the issue . Yeah , OK . Do you think that 's something I should just send to him Um {disfmarker} or do you think I should send it to this {disfmarker} there 's an {disfmarker} a m a mailing list . Well , it 's not a secret . I mean , we 're , you know , certainly willing to talk about it with everybody , but I think {disfmarker} I think that , um {disfmarker} um , it 's probably best to start talking with him just to {disfmarker} OK . Uh @ @ {comment} you know , it 's a dialogue between two of you about what {disfmarker} you know , what does he think about this and what {disfmarker} what {disfmarker} you know {disfmarker} what could be done about it . Yeah . OK . Um , if you get ten people in {disfmarker} involved in it there 'll be a lot of perspectives based on , you know , how {disfmarker} Yeah . you know . Right . Uh {disfmarker} But , I mean , I think it all should come up eventually , OK . but if {disfmarker} if {disfmarker} if there is any , uh , uh , way to move in {disfmarker} a way that would {disfmarker} that would , you know , be more open to different kinds of features . But if {disfmarker} if , uh {disfmarker} if there isn't , and it 's just kind of shut down and {disfmarker} and then also there 's probably not worthwhile bringing it into a larger forum where {disfmarker} where political issues will come in . Yeah . OK . Oh . So this is now {disfmarker} it 's {disfmarker} it 's compiled under Solaris ? Yeah . Yeah , OK . Yep . Because he {disfmarker} there was some mail r saying that it 's {disfmarker} may not be stable for Linux and all those . Yeah . Yeah , i that was a particular version . SUSI Yeah , SUSI or whatever it was yeah . Yeah , yeah . but we don't have that . Yeah , OK . So . Should be OK . OK , that 's fine . Yeah , it compiled fine actually . Yeah . No {disfmarker} no errors . Nothing . So . Uh , this is slightly off topic That 's good . but , uh , I noticed , just glancing at the , uh , Hopkins workshop , uh , web site that , uh , um {disfmarker} one of the thing I don't know {disfmarker} Well , we 'll see how much they accomplish , but one of the things that they were trying to do in the graphical models thing was to put together a {disfmarker} a , uh , tool kit for doing , uh r um , arbitrary graphical models for , uh , speech recognition . Hmm . So {disfmarker} And Jeff , uh {disfmarker} the two Jeffs were Who 's the second Jeff ? Uh {disfmarker} Oh , uh , do you know Geoff Zweig ? No . Oh . Uh , he {disfmarker} he , uh {disfmarker} he was here for a couple years Oh , OK . and he , uh {disfmarker} got his PHD . He {disfmarker} And he 's , uh , been at IBM for the last couple years . Oh , OK . So . Wow . That would be neat . Uh , so he did {disfmarker} he did his PHD on dynamic Bayes - nets , uh , for {disfmarker} for speech recognition . He had some continuity built into the model , presumably to handle some , um , inertia in the {disfmarker} in the production system , and , um {disfmarker} Hmm . So . Hmm . Um , I 've been playing with , first , the , um , VAD . Um , {vocalsound} so it 's exactly the same approach , but the features that the VAD neural network use are , uh , MFCC after noise compensation . Oh , I think I have the results . What was it using before ? Before it was just P L  So . Yeah , it was actually {disfmarker} No . Not {disfmarker} I mean , it was just the noisy features I guess . Yeah , Yeah , yeah , yeah , noisy {disfmarker} noisy features . not compensated . Um {disfmarker} This is what we get after {disfmarker} This {disfmarker} So , actually , we , yeah , here the features are noise compensated and there is also the LDA filter . Um , and then it 's a pretty small neural network which use , um , {vocalsound} nine frames of {disfmarker} of six features from C - zero to C - fives , plus the first derivatives . And it has one hundred hidden units . Is that nine frames u s uh , centered around the current frame ? Or {disfmarker} Yeah . Mm - hmm . S so , I 'm {disfmarker} I 'm sorry , there 's {disfmarker} there 's {disfmarker} there 's how many {disfmarker} how many inputs ? So it 's twelve times nine . Twelve times nine inputs , and a hundred , uh , hidden . Hidden and Two outputs . two outputs . Two outputs . OK . So I guess about eleven thousand parameters , which {disfmarker} actually shouldn't be a problem , even in {disfmarker} in small phones . Yeah . Mm - hmm . So , I 'm {disfmarker} I 'm {disfmarker} s so what is different between this and {disfmarker} and what you {disfmarker} It should be OK . So the previous syst It 's based on the system that has a fifty - three point sixty - six percent improvement . It 's the same system . The only thing that changed is the n a p eh {disfmarker} a es the estimation of the silence probabilities . Ah . OK . Which now is based on , uh , cleaned features . And , it 's a l it 's a lot better . Wow . Yeah . That 's great . Um {disfmarker} So it 's {disfmarker} it 's not bad , but the problem is still that the latency is too large . What 's the latency ? Because {disfmarker} um {disfmarker} the {disfmarker} the latency of the VAD is two hundred and twenty milliseconds . And , uh , the VAD is used uh , i for on - line normalization , and it 's used before the delta computation . So if you add these components it goes t to a hundred and seventy , right ? I {disfmarker} I 'm confused . You started off with two - twenty and you ended up with one - seventy ? With two an two hundred and seventy . Two - seventy . If {disfmarker} Yeah , if you add the c delta comp delta computation Oh . which is done afterwards . Um {disfmarker} So it 's two - twenty . I the is this {disfmarker} are these twenty - millisecond frames ? Is that why ? Is it after downsampling ? or {disfmarker} The two - twenty is one hundred milliseconds for the um {disfmarker} No , it 's forty milliseconds for t for the , uh , uh , cleaning of the speech . Um {disfmarker} then there is , um , the neural network which use nine frames . So it adds forty milliseconds . a OK . Um , after that , um , you have the um , filtering of the silence probabilities . Which is a million filter it , and it creates a one hundred milliseconds delay . So , um {disfmarker}  Plus there is a delta at the input . Yeah , and there is the delta at the input which is , One hundred milliseconds for smoothing . um {disfmarker} So it 's {disfmarker} @ @ {disfmarker} Uh , median .  It 's like forty plus {disfmarker} forty {disfmarker} plus {disfmarker} And then forty {disfmarker} Mmm . Forty {disfmarker} This forty plus twenty , plus one hundred . forty p  Uh {disfmarker} So it 's two hundred actually . Yeah , there are twenty that comes from {disfmarker} There is ten that comes from the LDA filters also . Right ? Oh , OK . Uh , so it 's two hundred and ten , yeah . If you are using {disfmarker} Uh {disfmarker} Plus the frame , t If you are using three frames {disfmarker} so it 's two - twenty . If you are phrasing f {comment} using three frames , it is thirty here for delta . Yeah , I think it 's {disfmarker} it 's five frames , but . So five frames , that 's twenty . OK , so it 's who un {comment} two hundred and ten . Uh , p Wait a minute . It 's forty {disfmarker} {vocalsound} forty for the {disfmarker} for the cleaning of the speech , So . Forty cleaning . forty for the I N {disfmarker} ANN , a hundred for the smoothing . Yeah . Well , but at ten {disfmarker} , Twenty for the delta . Twenty for delta . At th {nonvocalsound} At the input . I mean , that 's at the input to the net . Yeah . Delta at input to net ? And there i Yeah . Yeah . So it 's like s five , six cepstrum plus delta at nine {disfmarker} nine frames of {disfmarker} And then ten milliseconds for {disfmarker} Fi - There 's an LDA filter . ten milliseconds for LDA filter , and t and ten {disfmarker} another ten milliseconds you said for the frame ? For the frame I guess . I computed two - twenty {disfmarker} Yeah , well , it 's {disfmarker} I guess it 's for the fr {disfmarker} the {disfmarker} OK . And then there 's delta besides that ? So this is the features that are used by our network and then afterwards , you have to compute the delta on the , uh , main feature stream , OK . which is um , delta and double - deltas , which is fifty milliseconds . Yeah . No , I mean , the {disfmarker} after the noise part , the forty {disfmarker} the {disfmarker} the other hundred and eighty {disfmarker} Well , I mean , Wait a minute . Some of this is , uh {disfmarker} is , uh {disfmarker} is in parallel , isn't it ? I mean , the LDA {disfmarker} Oh , you have the LDA as part of the V D - uh , VAD ? Or {disfmarker} The VAD use , uh , LDA filtered features also . Oh , it does ? Mm - hmm . Ah . So in that case there isn't too much in parallel . Uh {disfmarker} No . There is , um , just downsampling , upsampling , and the LDA . Um , so the delta at the end is how much ? It 's fifty . It 's {disfmarker} Fifty . Alright . So {disfmarker} But well , we could probably put the delta , um , {vocalsound} before on - line normalization . It should not that make a big difference , What if you used a smaller window for the delta ? because {disfmarker} Could that help a little bit ? I mean , I guess there 's a lot of things you could do to {disfmarker} Yeah . Yeah . Yeah , So but , nnn {disfmarker} Yeah . So if you {disfmarker} if you put the delta before the , uh , ana on - line {disfmarker} If {disfmarker} Yeah {disfmarker} Mm - hmm . uh {disfmarker} then {disfmarker} then it could go in parallel . Cuz i And then y then you don't have that additive {disfmarker} Yeah , Yep . cuz the time constant of the on - line normalization is pretty long compared to the delta window , OK . so . It should not make {disfmarker} OK . And you ought to be able to shove tw , uh {disfmarker} sh uh {disfmarker} pull off twenty milliseconds from somewhere else to get it under two hundred , right ? I mean {disfmarker} Is two hundred the d The hundred milla Mm - hmm . mill a hundred milliseconds for smoothing is sort of an arbitrary amount . It could be eighty and {disfmarker} and probably do @ @ {disfmarker} Yeah , i a hun yeah . uh {disfmarker} Wh - what 's the baseline you need to be under ? Two hundred ? Well , we don't know . They 're still arguing about it .  Oh . I mean , if it 's two {disfmarker} if {disfmarker} if it 's , uh {disfmarker} if it 's two - fifty , then we could keep the delta where it is if we shaved off twenty . If it 's two hundred , if we shaved off twenty , we could {disfmarker} we could , uh , meet it by moving the delta back . So , how do you know that what you have is too much if they 're still deciding ? Uh , we don't , but it 's just {disfmarker} I mean , the main thing is that since that we got burned last time , and {disfmarker} you know , by not worrying about it very much , we 're just staying conscious of it . Uh - huh . Oh , OK , I see . And so , th I mean , if {disfmarker} if {disfmarker} if a week before we have to be done someone says , \" Well , you have to have fifty milliseconds less than you have now \" , it would be pretty frantic around here . So {disfmarker} Ah , OK . Uh {disfmarker} But still , that 's {disfmarker} that 's a pretty big , uh , win . And it doesn't seem like you 're {disfmarker} in terms of your delay , you 're , uh , that {disfmarker} He added a bit on , I guess , because before we were {disfmarker} we were {disfmarker} had {disfmarker} were able to have the noise , uh , stuff , uh , and the LVA be in parallel . Hmm . And now he 's {disfmarker} he 's requiring it to be done first . Well , but I think the main thing , maybe , is the cleaning of the speech , which takes forty milliseconds or so . Right . Well , so you say {disfmarker} let 's say ten milliseconds {disfmarker} seconds for the LDA . And {disfmarker} and {disfmarker} but {disfmarker} the LDA is , well , pretty short right now . Well , ten . And then forty for the other . Yeah . Yeah , the LDA {disfmarker} LDA {disfmarker} we don't know , is , like {disfmarker} is it very crucial for the features , right ? No . I just {disfmarker} This is the first try . Yeah . Right , I mean , I {disfmarker} maybe the LDA 's not very useful then . so you could start pulling back , S s h but {disfmarker} Yeah , But I think you have {disfmarker} l I mean , you have twenty for delta computation which y now you 're sort of doing twice , right ? But yo w were you doing that before ? Mmm . Well , in the proposal , um , the input of the VAD network were just three frames , I think . On the {disfmarker} in the {disfmarker} Mm - hmm . Just {disfmarker} Yeah , just the static , no delta . Right . Uh , static features . So , what you have now is fort uh , forty for the {disfmarker} the noise , twenty for the delta , and ten for the LDA . That 's seventy milliseconds of stuff which was formerly in parallel ,  right ? So I think , Mm - hmm . you know , that 's {disfmarker} that 's the difference as far as the timing , right ? Yeah . Um , and you could experiment with cutting various pieces of these back a bit , but {disfmarker} I mean , we 're s we 're not {disfmarker} we 're not in terrible shape . Yeah , that 's what it seems like to me . It 's pretty good . Yeah . Mm - hmm . It 's {disfmarker} it 's not like it 's adding up to four hundred milliseconds or something . Where {disfmarker} where is this {disfmarker} where is this fifty - seven point O two in {disfmarker} in comparison to the last evaluation ? Well , it 's {disfmarker} I think it 's better than anything , uh , anybody got . Oh , is that right ? Yeah . The best was fifty - four point five . Yeah . Point s Oh . Yeah . Uh And our system was forty - nine , but with the neural network . Wow . So this is almost ten percent . With the f with the neural net . Yeah , and r and {disfmarker} It would Yeah , so this is {disfmarker} this is like the first proposal . The proposal - one . It was forty - four , actually . Yeah . Yeah . And we still don't have the neural net in . So {disfmarker} so it 's {disfmarker} Wow . You know . So it 's {disfmarker} We 're {disfmarker} we 're doing better . This is {disfmarker} this is really good . I mean , we 're getting better recognition . I mean , I 'm sure other people working on this are not sitting still either , but {disfmarker} Yeah . but {disfmarker} but , uh {disfmarker} Uh , I mean , the important thing is that we learn how to do this better , and , you know . So . Um , Yeah . So , our , um {disfmarker} Yeah , you can see the kind of {disfmarker} kind of numbers that we 're having , say , on SpeechDat - Car which is a hard task , cuz it 's really , um {disfmarker} I think it 's just sort of {disfmarker} sort of reasonable numbers , starting to be . I mean , it 's still terri Mm - hmm . Yeah , even for a well - matched case it 's sixty percent error rate reduction , Yeah . which is {disfmarker} Yeah . Probably half . Good ! Um , Yeah . So actually , this is in between {vocalsound} what we had with the previous VAD and what Sunil did with an IDL VAD . Which gave sixty - two percent improvement , right ? Yeah , it 's almost that . So {disfmarker} It 's almost an average somewhere around {disfmarker} Yeah . Yeah . What was that ? Say that last part again ? So , if you use , like , an IDL VAD , uh , for dropping the frames , o o Or the best we can get . the best that we can get {disfmarker} i That means that we estimate the silence probability on the clean version of the utterances . Then you can go up to sixty - two percent error rate reduction , globally . Mmm . Mmm {disfmarker} Yeah . So that would be even {disfmarker} That wouldn't change this number down here to sixty - two ? Yeah . Yeah . So you {disfmarker} you were get If you add a g good v very good VAD , that works as well as a VAD working on clean speech , Yeah . Yeah . then you wou you would go {disfmarker} So that 's sort of the best you could hope for . Mm - hmm . I see . Probably . Yeah . So fi si fifty - three is what you were getting with the old VAD . Yeah . And , uh {disfmarker} and sixty - two with the {disfmarker} the , you know , quote , unquote , cheating VAD . And fifty - seven is what you got with the real VAD . Mm - hmm . That 's great . Uh , yeah , the next thing is , I started to play {disfmarker} Well , I don't want to worry too much about the delay , no . Maybe it 's better to wait OK . for the decision Yeah . from the committee . Uh , but I started to play with the , um , {vocalsound} {vocalsound} uh , tandem neural network . Mmm I just did the configuration that 's very similar to what we did for the February proposal . And {disfmarker} Um . So . There is a f a first feature stream that use uh straight MFCC features . Mm - hmm . Well , these features actually . And the other stream is the output of a neural network , using as input , also , these , um , cleaned MFCC . Um {disfmarker} Those are th those are th what is going into the tandem net ? I don't have the comp Mmm ? Those two ? So there is just this feature stream , {comment} the fifteen MFCC plus delta and double - delta . No . Yeah ? Um , so it 's {disfmarker} makes forty - five features {comment} that are used as input to the HTK . And then , there is {disfmarker} there are more inputs that comes from the tandem MLP . Oh , oh . OK . I see . Yeah , h he likes to use them both , Uh - huh . cuz then it has one part that 's discriminative , Yeah . Um {disfmarker} one part that 's not . Right . OK . So , um , uh , yeah . Right now it seems that {disfmarker} i I just tested on SpeechDat - Car while the experiment are running on your {disfmarker} on TI - digits . Well , it improves on the well - matched and the mismatched conditions , but it get worse on the highly mismatched . Um , Compared to these numbers ? Compared to these numbers , yeah . Um , y like , on the well - match and medium mismatch , the gain is around five percent relative , but it goes down a lot more , like fifteen percent on the HM case . You 're just using the full ninety features ?  The {disfmarker} Y you have ninety features ? i I have , um {disfmarker} From the networks , it 's twenty - eight . So {disfmarker} And from the other side it 's forty - five . So , d i It 's forty - five . So it 's {disfmarker} you have seventy - three features , Yeah . and you 're just feeding them like that . Yeah . There isn't any KLT or anything ? Mm - hmm . There 's a KLT after the neural network , as {disfmarker} as before . That 's how you get down to twenty - eight ? Yeah . Why twenty - eight ? I don't know . Oh . Uh . It 's {disfmarker} i i i It 's because it 's what we did for the first proposal . We tested , uh , trying to go down Ah . It 's a multiple of seven . and Yeah . Yeah . So {disfmarker} Um . Yeah . I wanted to do something very similar to the proposal as a first {disfmarker} first try . Yeah . I see . Yeah . Yeah . That makes sense . But we have to {disfmarker} for sure , we have to go down , because the limit is now sixty features . Yeah . So , uh , we have to find a way to decrease the number of features . Um {disfmarker} So , it seems funny that {disfmarker} I don't know , maybe I don't u quite understand everything , {comment} but that adding features {disfmarker} I guess {disfmarker} I guess if you 're keeping the back - end fixed . Maybe that 's it . Because it seems like just adding information shouldn't give worse results . But I guess if you 're keeping the number of Gaussians fixed in the recognizer , then {disfmarker} Well , yeah . Mmm . But , I mean , just in general , adding information {disfmarker} Suppose the information you added , well , was a really terrible feature and all it brought in was noise . Yeah . Right ? So {disfmarker} so , um {disfmarker} Or {disfmarker} or suppose it wasn't completely terrible , but it was completely equivalent to another one feature that you had , except it was noisier . Uh - huh . Right ? In that case you wouldn't necessarily expect it to be better at all . Oh , yeah , I wasn't necessarily saying it should be better . I 'm just surprised that you 're getting fifteen percent relative worse on the wel Uh - huh . But it 's worse . On the highly mismatched condition . On the highly mismatch . Yeah , I {disfmarker} Yeah . So , \" highly mismatched condition \" means that in fact your training is a bad estimate of your test . Uh - huh . So having {disfmarker} having , uh , a g a l a greater number of features , if they aren't maybe the right features that you use , certainly can e can easily , uh , make things worse . I mean , you 're right . If you have {disfmarker} if you have , uh , lots and lots of data , and you have {disfmarker} and your {disfmarker} your {disfmarker} your training is representative of your test , then getting more sources of information should just help . But {disfmarker} but it 's {disfmarker} It doesn't necessarily work that way . Huh . Mm - hmm . So I wonder , um , Well , what 's your {disfmarker} what 's your thought about what to do next with it ? Um , I don't know . I 'm surprised , because I expected the neural net to help more when there is more mismatch , as it was the case for the {disfmarker} Mm - hmm . So , was the training set same as the p the February proposal ? OK . Yeah , it 's the same training set , so it 's TIMIT with the TI - digits ' , uh , noises , uh , added .  Mm - hmm . Um {disfmarker} Well , we might {disfmarker} uh , we might have to experiment with , uh better training sets . Again . But , Mm - hmm . I {disfmarker} The other thing is , I mean , before you found that was the best configuration , but you might have to retest those things now that we have different {disfmarker} The rest of it is different , right ? So , um , uh , For instance , what 's the effect of just putting the neural net on without the o other {disfmarker} other path ? Mm - hmm . I mean , you know what the straight features do . Yeah . That gives you this . You know what it does in combination . Mm - hmm . You don't necessarily know what {disfmarker} What if you did the {disfmarker} Would it make sense to do the KLT on the full set of combined features ? Instead of just on the {disfmarker} Yeah . I g I guess . Um . The reason I did it this ways is that in February , it {disfmarker} we {disfmarker} we tested different things like that , so , having two KLT , having just a KLT for a network , or having a global KLT . Oh , I see . And {disfmarker} So you tried the global KLT before Well {disfmarker} and it didn't really {disfmarker} Yeah . And , uh , th Yeah . I see . The differences between these configurations were not huge , but {disfmarker} it was marginally better with this configuration . Uh - huh . Uh - huh . But , yeah , that 's obviously another thing to try , Um . since things are {disfmarker} things are different . Mm - hmm . Mm - hmm . And I guess if the {disfmarker} These are all {disfmarker} so all of these seventy - three features are going into , um , the , uh {disfmarker} the HMM . Yeah . And is {disfmarker} are {disfmarker} i i are {disfmarker} are any deltas being computed of tha of them ? Of the straight features , yeah . n Not of the {disfmarker} So . But n th the , um , tandem features are u used as they are . Are not . So , yeah , maybe we can add some context from these features also as {disfmarker} Dan did in {disfmarker} in his last work . Could . i Yeah , but the other thing I was thinking was , um {disfmarker} Uh , now I lost track of what I was thinking . But . What is the {disfmarker} You said there was a limit of sixty features or something ? Mm - hmm . What 's the relation between that limit and the , um , forty - eight {disfmarker} uh , forty eight hundred bits per second ? Oh , I know what I was gonna say . Um , not {disfmarker} no relation . No relation . So I {disfmarker} I {disfmarker} I don't understand , The f the forty - eight hundred bits is for transmission of some features . because i I mean , if you 're only using h And generally , i it {disfmarker} s allows you to transmit like , fifteen , uh , cepstrum . The issue was that , um , this is supposed to be a standard that 's then gonna be fed to somebody 's recognizer somewhere which might be , you know , it {disfmarker} it might be a concern how many parameters are use {disfmarker} u used and so forth . And so , uh , they felt they wanted to set a limit . So they chose sixty . Some people wanted to use hundreds of parameters and {disfmarker} and that bothered some other people . Uh - huh . u And so they just chose that . I {disfmarker} I {disfmarker} I think it 's kind of r arbitrary too . But {disfmarker} but that 's {disfmarker} that 's kind of what was chosen . I {disfmarker} I remembered what I was going to say . What I was going to say is that , um , maybe {disfmarker} {vocalsound} maybe with the noise removal , uh , these things are now more correlated . So you have two sets of things that are kind of uncorrelated , uh , within themselves , but they 're pretty correlated with one another . Mm - hmm . And , um , they 're being fed into these , uh , variants , only Gaussians and so forth , and {disfmarker} and , uh , Mm - hmm . so maybe it would be a better idea now than it was before to , uh , have , uh , one KLT over everything , to de - correlate it . Mm - hmm . Yeah , I see . Maybe . You know . What are the S N Rs in the training set , TIMIT ? It 's , uh , ranging from zero to clean ? Yeah . From zero to clean . Mm - hmm . Yeah . So we found this {disfmarker} this , uh {disfmarker} this Macrophone data , and so forth , that we were using for these other experiments , to be pretty good . Mm - hmm . So that 's {disfmarker} i after you explore these other alternatives , that might be another way to start looking , is {disfmarker} is just improving the training set . Mm - hmm . I mean , we were getting , uh , lots better recognition using that , than {disfmarker} Of course , you do have the problem that , um , u i {comment} we are not able to increase the number of Gaussians , uh , or anything to , uh , uh , to match anything . So we 're only improving the training of our feature set , but that 's still probably something . So you 're saying , add the Macrophone data to the training of the neural net ? The tandem net ? Yeah , that 's the only place that we can train . Yeah . We can't train the other stuff with anything other than the standard amount , Right . so . Um , um {disfmarker} What {disfmarker} what was it trained on again ? The one that you used ? It 's TIMIT with noise . Uh - huh . Yeah . So , yeah , it 's rather a small {disfmarker} How big is the net , by the way ? Um , Uh , it 's , uh , five hundred hidden units . And {disfmarker} And again , you did experiments back then where you made it bigger and it {disfmarker} and that was {disfmarker} that was sort of the threshold point . Much less than that , it was worse , Yeah . and Yeah . much more than that , it wasn't much better . Hmm . Yeah . @ @ ? So is it {disfmarker} is it though the performance , big relation in the high ma high mismatch has something to do with the , uh , cleaning up that you {disfmarker} that is done on the TIMIT after adding noise ?  So {disfmarker} it 's {disfmarker} i All the noises are from the TI - digits , Yeah . right ? So you {disfmarker} i Um {disfmarker} They {disfmarker} k uh {disfmarker} Well , it it 's like the high mismatch of the SpeechDat - Car after cleaning up , maybe having more noise than the {disfmarker} the training set of TIMIT after clean {disfmarker} s after you do the noise clean - up . Mmm . I mean , earlier you never had any compensation , you just trained it straight away . Mm - hmm . So it had like all these different conditions of S N Rs , actually in their training set of neural net . Mm - hmm . Mm - hmm . But after cleaning up you have now a different set of S N Rs , right ? Yeah . For the training of the neural net . Mm - hmm . And {disfmarker} is it something to do with the mismatch that {disfmarker} that 's created after the cleaning up , like the high mismatch {disfmarker} You mean the {disfmarker} the most noisy occurrences on SpeechDat - Car might be a lot more noisy than {disfmarker} Mm - hmm . Of {disfmarker} that {disfmarker} I mean , the SNR after the noise compensation of the SpeechDat - Car . Oh , so {disfmarker} Right . So the training {disfmarker} the {disfmarker} the neural net is being trained with noise compensated stuff . Maybe .  Yeah . Yeah , yeah . Which makes sense , Yeah . but , uh , you 're saying {disfmarker} Yeah , the noisier ones are still going to be , even after our noise compensation , are still gonna be pretty noisy . Yeah . Mm - hmm . Yeah , so now the after - noise compensation the neural net is seeing a different set of S N Rs than that was originally there in the training set . Of TIMIT . Because in the TIMIT it was zero to some clean . Right . Yes . So the net saw all the SNR @ @ conditions . Right . Now after cleaning up it 's a different set of SNR . Right . And that SNR may not be , like , com covering the whole set of S N Rs that you 're getting in the SpeechDat - Car . Right , but the SpeechDat - Car data that you 're seeing is also reduced in noise by the noise compensation . Yeah . Yeah , yeah , yeah , yeah , it is . But , I 'm saying , there could be some {disfmarker} some issues of {disfmarker} So . Mm - hmm . Yeah . Well , if the initial range of SNR is different , we {disfmarker} the problem was already there before . And {disfmarker} Yeah . Because {disfmarker} Mmm {disfmarker} Yeah , I mean , it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set . Hmm . Uh {disfmarker} On the test set , yeah .  Right ? I mean , you 're saying there 's a mismatch in noise that wasn't there before , Hmm . Mm - hmm . but if they were both the same before , then if they were both reduic reduced equally , then , there would not be a mismatch . Mm - hmm . So , I mean , this may be {disfmarker} Heaven forbid , this noise compensation process may be imperfect , but . Uh , so maybe it 's treating some things differently . Yeah , uh {disfmarker} Well , I {disfmarker} I don't know . I {disfmarker} I just {disfmarker} that could be seen from the TI - digits , uh , testing condition because , um , the noises are from the TI - digits , right ? Noise {disfmarker} Yeah . So {disfmarker} So cleaning up the TI - digits and if the performance goes down in the TI - digits mismatch {disfmarker} high mismatch like this {disfmarker} Clean training , yeah . on a clean training , or zero DB testing . Yeah , we 'll {disfmarker} so we 'll see . Uh . Yeah . Maybe . Then it 's something to do . Mm - hmm . I mean , one of the things about {disfmarker} Yeah . I mean , the Macrophone data , um , I think , you know , it was recorded over many different telephones . Mm - hmm . And , um , so , there 's lots of different kinds of acoustic conditions . I mean , it 's not artificially added noise or anything . So it 's not the same . I don't think there 's anybody recording over a car from a car , but {disfmarker} I think it 's {disfmarker} it 's varied enough that if {disfmarker} if doing this adjustments , uh , and playing around with it doesn't , uh , make it better , the most {disfmarker} uh , it seems like the most obvious thing to do is to improve the training set . Um {disfmarker} I mean , what we were {disfmarker} uh {disfmarker} the condition {disfmarker} It {disfmarker} it gave us an enormous amount of improvement in what we were doing with Meeting Recorder digits , even though there , again , these m Macrophone digits were very , very different from , uh , what we were going on here . I mean , we weren't talking over a telephone here . But it was just {disfmarker} I think just having a {disfmarker} a nice variation in acoustic conditions was just a good thing . Mm - hmm . Yep . Mmm . Yeah , actually {vocalsound} to s eh , what I observed in the HM case is that the number of deletion dramatically increases . It {disfmarker} it doubles . Number of deletions . When I added the num the neural network it doubles the number of deletions . Yeah , so I don't you know {vocalsound} how to interpret that , but , mmm {disfmarker} Yeah . Me either . t And {disfmarker} and did {disfmarker} an other numbers stay the same ? Insertion substitutions stay the same ? They p stayed the same , Roughly ? they {disfmarker} maybe they are a little bit uh , lower . Uh - huh . They are a little bit better . Yeah . But {disfmarker} Did they increase the number of deletions even for the cases that got better ? Mm - hmm . Say , for the {disfmarker} I mean , it {disfmarker} No , it doesn't . So it 's only the highly mismatched ? No . And it {disfmarker} Remind me again , the \" highly mismatched \" means that the {disfmarker} Clean training and {disfmarker} Uh , sorry ? It 's clean training {disfmarker} Well , close microphone training and distant microphone , um , high speed , I think . Close mike training {disfmarker} Well {disfmarker} The most noisy cases are the distant microphone for testing . Right . So {disfmarker} Well , maybe the noise subtraction is subtracting off speech . Separating . Yeah . Wh But {disfmarker} Yeah . I mean , but without the neural network it 's {disfmarker} well , it 's better . It 's just when we add the neural networks . Yeah , right . The feature are the same except that {disfmarker} Uh , that 's right , that 's right . Um {disfmarker} Well that {disfmarker} that says that , you know , the , um {disfmarker} the models in {disfmarker} in , uh , the recognizer are really paying attention to the neural net features . Yeah . Uh . Mm - hmm . But , yeah , actually {disfmarker} {nonvocalsound} the TIMIT noises {pause} are sort of a range of noises and they 're not so much the stationary driving kind of noises , right ? It 's {disfmarker} it 's pretty different . Isn't it ? Uh , there is a car noise . So there are f just four noises . Um , uh , \" Car \" , I think , \" Babble \" , \" Babble . \" \" Subway \" , right ? and {disfmarker} \" Street \" or \" Airport \" or something . and {disfmarker} \" Street \" isn't {disfmarker} Or \" Train station \" . \" Train station \" , yeah . Yeah . So {disfmarker} it 's mostly {disfmarker} Well , \" Car \" is stationary , Mm - hmm . \" Babble \" , it 's a stationary background plus some voices , Mm - hmm . some speech over it . And the other two are rather stationary also . Well , I {disfmarker} I think that if you run it {disfmarker} Actually , you {disfmarker} maybe you remember this . When you {disfmarker} in {disfmarker} in the old experiments when you ran with the neural net only , and didn't have this side path , um , uh , with the {disfmarker} the pure features as well , did it make things better to have the neural net ? Mm - hmm . Was it about the same ? Uh , w i It was {disfmarker} b a little bit worse . Than {disfmarker} ? Than just the features , yeah . So , until you put the second path in with the pure features , the neural net wasn't helping at all . Mm - hmm . Well , that 's interesting . It was helping , uh , if the features are b were bad , Yeah . I mean . Just plain P L Ps or M F Yeah . C Cs . as soon as we added LDA on - line normalization , and {vocalsound} all these things , then {disfmarker} They were doing similar enough things . Well , I still think it would be k sort of interesting to see what would happen if you just had the neural net without the side thing . Yeah , And {disfmarker} and the thing I {disfmarker} I have in mind is , uh , maybe you 'll see that the results are not just a little bit worse . mm - hmm . Maybe that they 're a lot worse . You know ? And , um {disfmarker} But if on the ha other hand , uh , it 's , say , somewhere in between what you 're seeing now and {disfmarker} and {disfmarker} and , uh , what you 'd have with just the pure features , then maybe there is some problem of a {disfmarker} of a , uh , combination of these things , or correlation between them somehow . Mm - hmm . If it really is that the net is hurting you at the moment , then I think the issue is to focus on {disfmarker} on , uh , improving the {disfmarker} the net . Yeah , Um . mm - hmm . So what 's the overall effe I mean , you haven't done all the experiments but you said it was i somewhat better , say , five percent better , for the first two conditions , and fifteen percent worse for the other one ? But it 's {disfmarker} but of course that one 's weighted lower , Y yeah , oh . Yeah . so I wonder what the net effect is . I d I {disfmarker} I think it 's {disfmarker} it was one or two percent . That 's not that bad , but it was l like two percent relative worse on SpeechDat - Car . I have to {disfmarker} to check that . Well , I have {disfmarker} I will . Well , it will {disfmarker} overall it will be still better even if it is fifteen percent worse , because the fifteen percent worse is given like f w twenty - five {disfmarker} point two five eight . Right . Mm - hmm . Hmm . Right . So the {disfmarker} so the worst it could be , if the others were exactly the same , is four , Is it like {disfmarker} and {disfmarker} and , uh , in fact since the others are somewhat better {disfmarker} Yeah , so it 's four . Is i So either it 'll get cancelled out , or you 'll get , like , almost the same . Uh . Yeah , it was {disfmarker} it was slightly worse . Slightly bad . Yeah . Um , Yeah , it should be pretty close to cancelled out . Yeah . You know , I 've been wondering about something . Mm - hmm . In the , um {disfmarker} a lot of the , um {disfmarker} the Hub - five systems , um , recently have been using LDA . and {disfmarker} and they , um {disfmarker} They run LDA on the features right before they train the models . So there 's the {disfmarker} the LDA is {disfmarker} is right there before the H M Yeah . So , you guys are using LDA but it seems like it 's pretty far back in the process . Uh , this LDA is different from the LDA that you are talking about . The LDA that you {disfmarker} saying is , like , you take a block of features , like nine frames or something , {comment} and then do an LDA on it , Yeah . Uh - huh . and then reduce the dimensionality to something like twenty - four or something like that . Yeah , you c you c you can . And then feed it to HMM . I mean , it 's {disfmarker} you know , you 're just basically i Yeah , so this is like a two d two dimensional tile . You 're shifting the feature space . Yeah . So this is a two dimensional tile . And the LDA that we are f applying is only in time , not in frequency {disfmarker} high cost frequency . So it 's like {disfmarker} more like a filtering in time , rather than doing a r Ah . OK . So what i what about , um {disfmarker} i u what i w I mean , I don't know if this is a good idea or not , but what if you put {disfmarker} ran the other kind of LDA , uh , on your features right before they go into the HMM ? Uh , it {disfmarker} Mm - hmm . No , actually , I think {disfmarker} i m Well . What do we do with the ANN is {disfmarker} is something like that except that it 's not linear . But it 's {disfmarker} it 's like a nonlinear discriminant analysis . Yeah . Right , it 's the {disfmarker} It 's {disfmarker} Right . The {disfmarker} So {disfmarker} Yeah , so it 's sort of like {disfmarker} But . The tandem stuff is kind of like i nonlinear LDA . Yeah . It 's {disfmarker} I g Yeah . Yeah . Yeah . But I mean , w but the other features that you have , um , th the non - tandem ones , Uh . Mm - hmm . Yeah , I know . That {disfmarker} that {disfmarker} Yeah . Well , in the proposal , they were transformed u using PCA , but {disfmarker} Uh - huh . Yeah , it might be that LDA could be better . The a the argument i is kind of i in {disfmarker} and it 's not like we really know , but the argument anyway is that , um , uh , we always have the prob I mean , discriminative things are good . LDA , neural nets , they 're good . Yeah . Uh , they 're good because you {disfmarker} you {disfmarker} you learn to distinguish between these categories that you want to be good at distinguishing between . And PCA doesn't do that . It {disfmarker} PAC - PCA {disfmarker} low - order PCA throws away pieces that are uh , maybe not {disfmarker} not gonna be helpful just because they 're small , basically . Right . But , uh , the problem is , training sets aren't perfect and testing sets are different . So you f you {disfmarker} you face the potential problem with discriminative stuff , be it LDA or neural nets , that you are training to discriminate between categories in one space but what you 're really gonna be g getting is {disfmarker} is something else . Uh - huh . And so , uh , Stephane 's idea was , uh , let 's feed , uh , both this discriminatively trained thing and something that 's not . So you have a good set of features that everybody 's worked really hard to make , Yeah . and then , uh , you {disfmarker} you discriminately train it , but you also take the path that {disfmarker} that doesn't have that , Uh - huh . and putting those in together . And that {disfmarker} that seem So it 's kind of like a combination of the {disfmarker} uh , what , uh , Dan has been calling , you know , a feature {disfmarker} uh , you know , a feature combination versus posterior combination or something . It 's {disfmarker} it 's , you know , you have the posterior combination but then you get the features from that and use them as a feature combination with these {disfmarker} these other things . And that seemed , at least in the last one , as he was just saying , he {disfmarker} he {disfmarker} when he only did discriminative stuff , i it actually was {disfmarker} was {disfmarker} it didn't help at all in this particular case . Yeah . There was enough of a difference , I guess , between the testing and training . But by having them both there {disfmarker} The fact is some of the time , the discriminative stuff is gonna help you . Mm - hmm . And some of the time it 's going to hurt you , Right . and by combining two information sources if , you know {disfmarker} if {disfmarker} if {disfmarker} So you wouldn't necessarily then want to do LDA on the non - tandem features because now you 're doing something to them that {disfmarker} That i i I think that 's counter to that idea . Yeah , right . Now , again , it 's {disfmarker} we 're just trying these different things . We don't really know what 's gonna work best . But if that 's the hypothesis , at least it would be counter to that hypothesis to do that . Right . Um , and in principle you would think that the neural net would do better at the discriminant part than LDA . Right . Yeah . Well {disfmarker} y Though , maybe not . Yeah . Exactly . I mean , we , uh {disfmarker} we were getting ready to do the tandem , uh , stuff for the Hub - five system , and , um , Andreas and I talked about it , and the idea w the thought was , \" Well , uh , yeah , that i you know {disfmarker} th the neural net should be better , but we should at least have uh , a number , you know , to show that we did try the LDA in place of the neural net , so that we can you know , show a clear path . Right . You know , that you have it without it , then you have the LDA , then you have the neural net , and you can see , theoretically . So . I was just wondering {disfmarker} I {disfmarker} I {disfmarker} Well , I think that 's a good idea . Yeah . Did {disfmarker} did you do that Um . No . or {disfmarker} tha that 's a {disfmarker} That 's what {disfmarker} that 's what we 're gonna do next as soon as I finish this other thing . So . Yeah . Yeah . No , well , that 's a good idea . I {disfmarker} I {disfmarker} We just want to show . i Yeah . I mean , it {disfmarker} everybody believes it , Oh , no it 's a g but you know , we just {disfmarker} No , no , but it might not {disfmarker} not even be true . Yeah . I mean , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's a great idea . I mean , one of the things that always disturbed me , uh , in the {disfmarker} the resurgence of neural nets that happened in the eighties was that , um , a lot of people {disfmarker} Because neural nets were pretty easy to {disfmarker} to use {disfmarker} a lot of people were just using them for all sorts of things without , uh , looking at all into the linear , uh {disfmarker} uh , versions of them . Yeah . Mm - hmm . Yeah . And , uh , people were doing recurrent nets but not looking at IIR filters , and {disfmarker} You know , I mean , uh , so I think , yeah , it 's definitely a good idea to try it . Yeah , and everybody 's putting that on their {vocalsound} systems now , and so , I that 's what made me wonder about this , Well , they 've been putting them in their systems off and on for ten years , but . but {disfmarker} but {disfmarker} but , uh , Yeah , what I mean is it 's {disfmarker} it 's like in the Hub - five evaluations , you know , and you read the system descriptions and everybody 's got , {vocalsound} you know , LDA on their features . And now they all have that . I see . And so . Yeah . Uh . It 's the transformation they 're estimating on {disfmarker} Well , they are trained on the same data as the final HMM are . Yeah , so it 's different . Yeah , exactly . Cuz they don't have these , you know , mismatches that {disfmarker} that you guys have . Mm - hmm . So that 's why I was wondering if maybe it 's not even a good idea . Mm - hmm . I don't know . I {disfmarker} I don't know enough about it , Mm - hmm . but {disfmarker} Um . I mean , part of why {disfmarker} I {disfmarker} I think part of why you were getting into the KLT {disfmarker} Y you were describing to me at one point that you wanted to see if , uh , you know , getting good orthogonal features was {disfmarker} and combining the {disfmarker} the different temporal ranges {disfmarker} was the key thing that was happening or whether it was this discriminant thing , right ? So you were just trying {disfmarker} I think you r I mean , this is {disfmarker} it doesn't have the LDA aspect but th as far as the orthogonalizing transformation , you were trying that at one point , right ? Mm - hmm . I think you were . Mm - hmm . Yeah . Does something . It doesn't work as well . Yeah . Yeah . So , yeah , I 've been exploring a parallel VAD without neural network with , like , less latency using SNR and energy , um , after the cleaning up . So what I 'd been trying was , um , uh {disfmarker} After the b after the noise compensation , n I was trying t to f find a f feature based on the ratio of the energies , that is , cl after clean and before clean . So that if {disfmarker} if they are , like , pretty c close to one , which means it 's speech . And if it is n if it is close to zero , which is {disfmarker} So it 's like a scale @ @ probability value . So I was trying , uh , with full band and multiple bands , m ps uh {disfmarker} separating them to different frequency bands and deriving separate decisions on each bands , and trying to combine them . Uh , the advantage being like it doesn't have the latency of the neural net if it {disfmarker} if it can Mm - hmm . g And {pause} it gave me like , uh , one point {disfmarker} One {disfmarker} more than one percent relative improvement . So , from fifty - three point six it went to fifty f four point eight . So it 's , like , only slightly more than a percent improvement , Mm - hmm . just like {disfmarker} Which means that it 's {disfmarker} it 's doing a slightly better job than the previous VAD , Mm - hmm . uh , at a l lower delay . Mm - hmm . Um , so , um {disfmarker} But {disfmarker} i d I 'm sorry , so {disfmarker} u does it still have the median {pause} filter stuff ? It still has the median filter . So it still has most of the delay , So {disfmarker} it just doesn't {disfmarker} Yeah , so d with the delay , that 's gone is the input , which is the sixty millisecond . The forty plus {pause} twenty . Well , w i At the input of the neural net you have this , uh , f nine frames of context plus the delta . Oh , plus the delta , Mm - hmm . right . OK . Yeah . So that delay , plus the LDA . Mm - hmm . Uh , so the delay is only the forty millisecond of the noise cleaning , plus the hundred millisecond smoothing at the output . Mm - hmm . Mm - hmm . Um . So . Yeah . So the {disfmarker} the {disfmarker} di the biggest {disfmarker} The problem f for me was to find a consistent threshold that works {pause} well across the different databases , because I t I try to make it work on tr SpeechDat - Car Mm - hmm . and it fails on TI - digits , or if I try to make it work on that it 's just the Italian or something , it doesn't work on the Finnish . Mm - hmm . So , um . So there are {disfmarker} there was , like , some problem in balancing the deletions and insertions when I try different thresholds . Mm - hmm . So {disfmarker} The {disfmarker} I 'm still trying to make it better by using some other features from the {disfmarker} after the p clean up {disfmarker} maybe , some , uh , correlation {disfmarker} auto - correlation or some s additional features of {disfmarker} to mainly the improvement of the VAD . I 've been trying . Now this {disfmarker} this {disfmarker} this , uh , \" before and after clean \" , it sounds like you think that 's a good feature . That {disfmarker} that , it {disfmarker} you th think that the , uh {disfmarker} the {disfmarker} i it appears to be a good feature , right ? Mm - hmm . What about using it in the neural net ? Yeah . Yeah , eventually we could {disfmarker} could just Yeah , so {disfmarker} Yeah , so that 's the {disfmarker} Yeah . So we 've been thinking about putting it into the neural net also . Yeah . Because they did {disfmarker} that itself {disfmarker} Then you don't have to worry about the thresholds and {disfmarker} There 's a threshold and {disfmarker} Yeah . Yeah . but just {disfmarker} Yeah . So that {disfmarker} that 's , uh {disfmarker} Yeah . So if we {disfmarker} if we can live with the latency or cut the latencies elsewhere , then {disfmarker} then that would be a , uh , good thing . Yeah . Yeah . Um , anybody {disfmarker} has anybody {disfmarker} you guys or {disfmarker} or Naren , uh , somebody , tried the , uh , um , second th second stream thing ? Uh . Oh , I just {disfmarker} I just h put the second stream in place and , uh ran one experiment , but just like {disfmarker} just to know that everything is fine . Uh - huh . So it was like , uh , forty - five cepstrum plus twenty - three mel {disfmarker} log mel . Yeah . And {disfmarker} and , just , like , it gave me the baseline performance of the Aurora , which is like zero improvement . Yeah . Yeah . So I just tried it on Italian just to know that everything is {disfmarker} But I {disfmarker} I didn't export anything out of it because it was , like , a weird feature set . Yeah . So . Yeah . Well , what I think , you know , would be more what you 'd want to do is {disfmarker} is {disfmarker} is , uh , put it into another neural net . Right ? Mm - hmm . Yeah , yeah , yeah , yeah . And then {disfmarker} But , yeah , we 're {disfmarker} we 're not quite there yet . So we have to {vocalsound} figure out the neural nets , I guess . Yeah . The uh , other thing I was wondering was , um , if the neural net , um , has any {disfmarker} because of the different noise con unseen noise conditions for the neural net , where , like , you train it on those four noise conditions , while you are feeding it with , like , a additional {disfmarker} some four plus some {disfmarker} f few more conditions which it hasn't seen , actually , Mm - hmm . from the {disfmarker} f f while testing . Yeah , yeah . Right . Um {disfmarker} instead of just h having c uh , those cleaned up t cepstrum , sh should we feed some additional information , like {disfmarker} The {disfmarker} the {disfmarker} We have the VAD flag . I mean , should we f feed the VAD flag , also , at the input so that it {disfmarker} it has some additional discriminating information at the input ? Hmm - hmm ! Um {disfmarker} Wh - uh , the {disfmarker} the VAD what ? We have the VAD information also available at the back - end . Uh - huh . So if it is something the neural net is not able to discriminate the classes {disfmarker} Yeah . I mean {disfmarker} Because most of it is sil I mean , we have dropped some silence f We have dropped so silence frames ? Mm - hmm . No , we haven't dropped silence frames still . Uh , still not . Yeah . Yeah . So {disfmarker} Th the b b biggest classification would be the speech and silence . So , by having an additional , uh , feature which says \" this is speech and this is nonspeech \" , I mean , it certainly helps in some unseen noise conditions for the neural net . What {disfmarker} Do y do you have that feature available for the test data ? Well , I mean , we have {disfmarker} we are transferring the VAD to the back - end {disfmarker} feature to the back - end . Because we are dropping it at the back - end after everything {disfmarker} all the features are computed . Oh , oh , I see . So {disfmarker} I see . so the neural {disfmarker} so that is coming from a separate neural net or some VAD . OK . OK . Which is {disfmarker} which is certainly giving a So you 're saying , feed that , also , into {pause} the neural net . to {disfmarker} Yeah . So it it 's an {disfmarker} additional discriminating information . Yeah . Yeah . Right . So that {disfmarker} You could feed it into the neural net . The other thing {comment} you could do is just , um , p modify the , uh , output probabilities of the {disfmarker} of the , uh , uh , um , neural net , tandem neural net , {comment} based on the fact that you have a silence probability . Mm - hmm . Right ? Mm - hmm . So you have an independent estimator of what the silence probability is , and you could multiply the two things , and renormalize . Yeah . Uh , I mean , you 'd have to do the nonlinearity part and deal with that . Uh , I mean , go backwards from what the nonlinearity would , you know {disfmarker} would be . Through {disfmarker} t to the soft max . But {disfmarker} but , uh {disfmarker} Yeah , so {disfmarker} maybe , yeah , when {disfmarker} But in principle wouldn't it be better to feed it in ? And let the net do that ? Well , u Not sure . Hmm . I mean , let 's put it this way . I mean , y you {disfmarker} you have this complicated system with thousands and thousand parameters Yeah . and you can tell it , uh , \" Learn this thing . \" Or you can say , \" It 's silence ! Go away ! \" I mean , I mean , i Doesn't {disfmarker} ? I think {disfmarker} I think the second one sounds a lot more direct . What {disfmarker} what if you {disfmarker} Uh . Right . So , what if you then , uh {disfmarker} since you know this , what if you only use the neural net on the speech portions ? Well , uh , That 's what {disfmarker} Well , I guess that 's the same . Uh , that 's similar . Yeah , I mean , y you 'd have to actually run it continuously , But I mean {disfmarker} I mean , train the net only on {disfmarker} but it 's {disfmarker} @ @ {disfmarker} Well , no , you want to train on {disfmarker} on the nonspeech also , because that 's part of what you 're learning in it , to {disfmarker} to {disfmarker} to generate , that it 's {disfmarker} it has to distinguish between . Speech . But I mean , if you 're gonna {disfmarker} if you 're going to multiply the output of the net by this other decision , uh , would {disfmarker} then you don't care about whether the net makes that distinction , right ? Well , yeah . But this other thing isn't perfect . Ah . So that you bring in some information from the net itself . Right , OK . That 's a good point . Yeah . Now the only thing that {disfmarker} that bothers me about all this is that I {disfmarker} I {disfmarker} I {disfmarker} The {disfmarker} the fact {disfmarker} i i It 's sort of bothersome that you 're getting more deletions . Yeah . But {disfmarker} So I might maybe look at , is it due to the fact that um , the probability of the silence at the output of the network , is , uh , Is too high . too {disfmarker} too high or {disfmarker} Yeah . So maybe {disfmarker} So {disfmarker} If it 's the case , then multiplying it again by {disfmarker} i by something ? It may not be {disfmarker} it {disfmarker} Yeah . Mm - hmm . Yeah , it {disfmarker} it may be too {disfmarker} it 's too high in a sense , like , everything is more like a , um , flat probability . Yeah . Oh - eee - hhh . So , like , it 's not really doing any distinction between speech and nonspeech {disfmarker} Uh , yeah . or , I mean , different {disfmarker} among classes . Yeah . Mm - hmm . Be interesting to look at the {disfmarker} Yeah , for the {disfmarker} I wonder if you could do this . But if you look at the , um , highly mism high mismat the output of the net on the high mismatch case and just look at , you know , the distribution versus the {disfmarker} the other ones , do you {disfmarker} do you see more peaks or something ? Yeah . Yeah , like the entropy of the {disfmarker} the output , Yeah . Yeah , for instance . or {disfmarker} But I {disfmarker} bu It {disfmarker} it seems that the VAD network doesn't {disfmarker} Well , it doesn't drop , uh , too many frames because the dele the number of deletion is reasonable . But it 's just when we add the tandem , the final MLP , and then {disfmarker} Yeah . Now the only problem is you don't want to ta I guess wait for the output of the VAD before you can put something into the other system , u cuz that 'll shoot up the latency a lot , right ? Am I missing something here ? But {disfmarker} Mm - hmm . Yeah . Right . Yeah . So that 's maybe a problem with what I was just saying . But {disfmarker} but {disfmarker} I I guess {disfmarker} But if you were gonna put it in as a feature it means you already have it by the time you get to the tandem net , right ? Um , well . We {disfmarker} w we don't have it , actually , No . because it 's {disfmarker} it has a high rate energy {disfmarker} Ah . the VAD has a {disfmarker} Yeah . OK . It 's kind of done in {disfmarker} I mean , some of the things are , not in parallel , but certainly , it would be in parallel with the {disfmarker} with a tandem net . Right . In time . So maybe , if that doesn't work , um {disfmarker} But it would be interesting to see if that was the problem , anyway . And {disfmarker} and {disfmarker} and then I guess another alternative would be to take the feature that you 're feeding into the VAD , and feeding it into the other one as well . Mm - hmm . And then maybe it would just learn {disfmarker} learn it better . Mm - hmm . Um {disfmarker} But that 's {disfmarker} Yeah , that 's an interesting thing to try to see , if what 's going on is that in the highly mismatched condition , it 's , um , causing deletions by having this silence probability up {disfmarker} up too high , Mm - hmm . at some point where the VAD is saying it 's actually speech . Yeah . Which is probably true . So , m Cuz {disfmarker} Well , the V A if the VAD said {disfmarker} since the VAD is {disfmarker} is {disfmarker} is right a lot , uh {disfmarker} Yeah . Hmm . Anyway . Might be . Mm - hmm . Yeah . Well , we just started working with it . But these are {disfmarker} these are some good ideas I think . Mm - hmm . Yeah , and the other thing {disfmarker} Well , there are other issues maybe for the tandem , like , uh , well , do we want to , w uh n Do we want to work on the targets ? Or , like , instead of using phonemes , using more context dependent units ? For the tandem net you mean ? Well , I 'm {disfmarker} Yeah . Hmm . I 'm thinking , also , a w about Dan 's work where he {disfmarker} he trained {vocalsound} a network , not on phoneme targets but on the HMM state targets . And {disfmarker} it was giving s slightly better results . Problem is , if you are going to run this on different m test sets , including large vocabulary , Yeah . Yeah . um , Uh {disfmarker} I think {disfmarker} Mmm . I was just thinking maybe about , like , generalized diphones , and {disfmarker} come up with a {disfmarker} a reasonable , not too large , set of context dependent units , and {disfmarker} and {disfmarker} Yeah . And then anyway we would have to reduce this with the KLT . Yeah . So . But {disfmarker} I don't know . Yeah . Well , maybe . But I d I d it {disfmarker} it {disfmarker} i it 's all worth looking at , Mm - hmm . but it sounds to me like , uh , looking at the relationship between this and the {disfmarker} speech noise stuff is {disfmarker} is {disfmarker} is probably a key thing . Mm - hmm . That and the correlation between stuff . So if , uh {disfmarker} if the , uh , high mismatch case had been more like the , uh , the other two cases {comment} in terms of giving you just a better performance , {comment} how would this number have changed ? Mm - hmm . Oh , it would be {disfmarker} Yeah . Around five percent better , I guess . If {disfmarker} if {disfmarker} i y Like sixty ? Well , we don't know what 's it 's gonna be the TI - digits yet . He hasn't got the results back yet . Yeah . If you extrapolate the SpeechDat - Car well - matched and medium - mismatch , it 's around , yeah , maybe five . Uh - huh . Yeah . So this would be sixty - two ? Sixty - two . Which is {disfmarker} Yeah . Sixty - two , yeah . Somewhere around sixty , must be . Right ? Yeah . Well , it 's around five percent , because it 's {disfmarker} s Right ? If everything is five percent . Yeah . Yeah . All the other ones were five percent , Mm - hmm . the {disfmarker} Yeah . I d I d I just have the SpeechDat - Car right now , so {disfmarker} Yeah . It 's running {disfmarker} it shou we should have the results today during the afternoon , Hmm . but {disfmarker} Well . Hmm . Well {disfmarker} Um {disfmarker} So I won't be here for {disfmarker} When {disfmarker} When do you leave ? Uh , I 'm leaving next Wednesday . May or may not be in in the morning . I leave in the afternoon . Um , But you 're {disfmarker} so I {disfmarker} are you {disfmarker} you 're not gonna be around this afternoon ? Yeah . Oh . Oh , well . I 'm talking about next week . I 'm leaving {disfmarker} leaving next Wednesday . Uh - huh . This afternoon {disfmarker} uh {disfmarker} Oh , right , for the Meeting meeting ? Yeah , that 's just cuz of something on campus . Ah , OK , OK . Yeah . But , um , yeah , so next week I won't , and the week after I won't , cuz I 'll be in Finland . And the week after that I won't . By that time you 'll be {disfmarker} {comment} Uh , you 'll both be gone {pause} from here . So there 'll be no {disfmarker} definitely no meeting on {disfmarker} on September sixth . Uh , What 's September sixth ? and {disfmarker} Uh , that 's during Eurospeech . Oh , oh , right . OK . So , uh , Sunil will be in Oregon . Uh , Stephane and I will be in Denmark . Uh {disfmarker} Right ? So it 'll be a few weeks , really , before we have a meeting of the same cast of characters . Um , but , uh {disfmarker} I guess , just {disfmarker} I mean , you guys should probably meet . And maybe Barry {disfmarker} Barry will be around . And {disfmarker} and then uh , uh , we 'll start up again with Dave and {disfmarker} Dave and Barry and Stephane and us on the , uh , twentieth . No . Thirteenth ? About a month ? So , uh , you 're gonna be gone for the next three weeks or something ? I 'm gone for two and a half weeks starting {disfmarker} starting next Wed - late next Wednesday . So that 's {disfmarker} you won't be at the next three of these meetings . Is that right ? Uh , I won't {disfmarker} it 's probably four because of {disfmarker} is it three ? Let 's see , twenty - third , thirtieth , sixth . That 's right , next three . And the {disfmarker} the third one won't {disfmarker} probably won't be a meeting , cuz {disfmarker} cuz , uh , Su - Sunil , Stephane , and I will all not be here . Oh , right . Right . Um {disfmarker} Mmm . {comment} So it 's just , uh , the next two where there will be {disfmarker} there , you know , may as well be meetings , OK . but I just won't be at them . And then starting up on the thirteenth , {nonvocalsound} uh , we 'll have meetings again but we 'll have to do without Sunil here somehow . When do you go back ? So . Thirty - first , August . Yeah . Yeah . So . Cool . When is the evaluation ? November , or something ? Yeah , it was supposed to be November fifteenth . Has anybody heard anything different ? I don't know . The meeting in {disfmarker} is the five and six of December . So {disfmarker} p s It 's like {disfmarker} Yeah , it 's tentatively all full . Yeah . Mm - hmm . Uh , that 's a proposed date , I guess . Yeah , um {disfmarker} so the evaluation should be on a week before or {disfmarker} Yeah . Yep . But , no , this is good progress . So . Uh {disfmarker} OK . Should we do digits ? Guess we 're done . Digits ? Yep . OK . It 's a wrap .",
        "summarize": "The team was coming close to finalizing the model for Aurora. They were still trying to make some improvements to improve their score. The team began with a discussion about how reverberation could be better accounted for. The professor thought that experimenting with different filters could help achieve a better signal-to-noise ratio. The new system for estimating silence probabilities that the team had added to the model was very effective, but it created a 220ms latency in the VAD. The team was not sure what kinds of constraints would be placed on latency eventually. They wanted to play it safe. The models, over all, were performing well, though the team intended to keep improving them. The team was also trying to figure out how to deal with different kinds of background noise. The meeting ended with a discussion of logistical issues."
    }
]
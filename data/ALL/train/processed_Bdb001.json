[
    {
        "transcript": "Yeah , we had a long discussion about how much w how easy we want to make it for people to bleep things out . So {disfmarker} Morgan wants to make it hard . It {disfmarker} it doesn't {disfmarker} Did {disfmarker} did {disfmarker} did it {disfmarker} ? I didn't even check yesterday whether it was moving . It didn't move yesterday either when I started it . So . So I don't know if it doesn't like both of us {disfmarker} Channel three ? Channel three ? You know , I discovered something yesterday on these , um , wireless ones . Channel two . Mm - hmm ? You can tell if it 's picking up {pause} breath noise and stuff . Yeah , it has a little indicator on it {disfmarker} on the AF . Mm - hmm . So if you {disfmarker} yeah , if you breathe under {disfmarker} breathe and then you see AF go off , then you know {pause} it 's p picking up your mouth noise . Oh , that 's good . Cuz we have a lot of breath noises . Yep . Test . In fact , if you listen to just the channels of people not talking , it 's like \" @ @ \" . It 's very disgust What ? Did you see Hannibal recently or something ? Sorry . Exactly . It 's very disconcerting . OK . So , um ,  I was gonna try to get out of here , like , in half an hour , um , cuz I really appreciate people coming , and {vocalsound} the main thing that I was gonna ask people to help with today is {pause} to give input on what kinds of database format we should {pause} use in starting to link up things like word transcripts and annotations of word transcripts , so anything that transcribers or discourse coders or whatever put in the signal , {vocalsound} with time - marks for , like , words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer . So , we have this , um {disfmarker} I think a starting point is clearly the {disfmarker} the channelized {pause} output of Dave Gelbart 's program , which Don brought a copy of , Yeah . Yeah , I 'm {disfmarker} I 'm familiar with that . I mean , we {disfmarker} I sort of already have developed an XML format for this sort of stuff . um , which {disfmarker} Can I see it ? And so the only question {disfmarker} is it the sort of thing that you want to use or not ? Have you looked at that ? I mean , I had a web page up . Right . So , So {disfmarker} I actually mostly need to be able to link up , or {disfmarker} I it 's {disfmarker} it 's a question both of what the representation is and {disfmarker} You mean , this {disfmarker} I guess I am gonna be standing up and drawing on the board . OK , yeah . So you should , definitely . Um , so {disfmarker} so it definitely had that as a concept . So tha it has a single time - line , Mm - hmm . and then you can have lots of different sections , each of which have I Ds attached to it , and then you can refer from other sections to those I Ds , if you want to . So that , um {disfmarker} so that you start with {disfmarker} with a time - line tag . \" Time - line \" . And then you have a bunch of times . I don't e I don't remember exactly what my notation was , Oh , I remember seeing an example of this . but it {disfmarker} Right , right . Yeah . Yeah , \" T equals one point three two \" , uh {disfmarker} And then I {disfmarker} I also had optional things like accuracy , and then \" ID equals T one , uh , one seven \" . And then , {nonvocalsound} I also wanted to {disfmarker} to be i to be able to not specify specifically what the time was and just have a stamp . Right . Yeah , so these are arbitrary , assigned by a program , not {disfmarker} not by a user . So you have a whole bunch of those . And then somewhere la further down you might have something like an utterance tag which has \" start equals T - seventeen , end equals T - eighteen \" . So what that 's saying is , we know it starts at this particular time . We don't know when it ends . OK . Right ? But it ends at this T - eighteen , which may be somewhere else . We say there 's another utterance . We don't know what the t time actually is but we know that it 's the same time as this end time . Mmm . You know , thirty - eight , whatever you want . So you 're essentially defining a lattice . OK . Yes , exactly . Yeah . And then , uh {disfmarker} and then these also have I Ds . Right ? So you could {disfmarker} you could have some sort of other {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh , {nonvocalsound} \" noise - type equals {nonvocalsound} door - slam \" . You know ? And then , uh , {nonvocalsound} you could either say \" time equals a particular time - mark \" or you could do other sorts of references . So {disfmarker} or {disfmarker} or you might have a prosody {disfmarker} \" Prosody \" right ? D ? T ? D ? T ? T ? It 's an O instead of an I , but the D is good . You like the D ? That 's a good D . Yeah . Um , you know , so you could have some sort of type here , and then you could have , um {disfmarker} the utterance that it 's referring to could be U - seventeen or something like that . OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and , Oh , well . um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ? How - how I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may or may not change {disfmarker} ? Oh , that 's {disfmarker} That 's actually very nicely handled here because you could {disfmarker} you could {disfmarker} all you 'd have to change is the , {vocalsound} um , time - stamps in the time - line without {disfmarker} without , uh , changing the I Ds . Um . And you 'd be able to propagate all of the {disfmarker} the information ? Right . That 's , the who that 's why you do that extra level of indirection . So that you can just change the time - line . Except the time - line is gonna be huge . If you say {disfmarker} Yes . Yeah , suppose you have a phone - level alignment . yeah , especially at the phone - level . You 'd have {disfmarker} you 'd have {disfmarker} The {disfmarker} we {disfmarker} we have phone - level backtraces . Yeah , this {disfmarker} I don't think I would do this for phone - level . I think for phone - level you want to use some sort of binary representation Um {disfmarker} because it 'll be too dense otherwise . OK . So , if you were doing that and you had this sort of companion , uh , thing that gets called up for phone - level , uh , what would that look like ? Why I would use just an existing {disfmarker} an existing way of doing it . How would you {disfmarker} ? Mmm . But {disfmarker} but why not use it for phone - level ? H h It 's just a matter of {disfmarker} it 's just a matter of it being bigger . But if you have {disfmarker} you know , barring memory limitations , or uh {disfmarker} I w I mean this is still the m It 's parsing limitations . I don't want to have this text file that you have to read in the whole thing to do something very simple for . Oh , no . You would use it only {pause} for {pause} purposes where you actually want the phone - level information , I 'd imagine . So you could have some file that configures how much information you want in your {disfmarker} in your XML or something . Right . I mean , you 'd {disfmarker} y Um , You {disfmarker} I {disfmarker} I am imagining you 'd have multiple versions of this depending on the information that you want . cuz th it does get very bush with {disfmarker} Right . Um , I 'm just {disfmarker} what I 'm wondering is whether {disfmarker} I think for word - level , this would be OK . Yeah . For word - level , it 's alright . Yeah . Definitely . Mm - hmm . For lower than word - level , you 're talking about so much data that I just {disfmarker} I don't know . I don't know if that {disfmarker} I mean , we actually have {disfmarker} So , one thing that Don is doing , is we 're {disfmarker} we 're running {disfmarker} For every frame , you get a pitch value , Lattices are big , too . and not only one pitch value but different kinds of pitch values Yeah , I mean , for something like that I would use P - file depending on {disfmarker} or {disfmarker} or any frame - level stuff I would use P - file . Meaning {disfmarker} ? Uh , that 's a {disfmarker} well , or something like it . It 's ICS uh , ICSI has a format for frame - level representation of features . Um . OK . That you could call {disfmarker} that you would tie into this representation with like an ID . Right . Right . Or {disfmarker} or there 's a {disfmarker} there 's a particular way in XML to refer to external resources . And {disfmarker} OK . So you would say \" refer to this external file \" . Um , so that external file wouldn't be in {disfmarker} So that might {disfmarker} that might work . But what {disfmarker} what 's the advantage of doing that versus just putting it into this format ? More compact , which I think is {disfmarker} is better . Uh - huh . I mean , if you did it at this {disfmarker} I mean these are long meetings and with {disfmarker} for every frame , You don't want to do it with that {disfmarker} Anything at frame - level you had better encode binary um {disfmarker} or it 's gonna be really painful . Or you just compre I mean , I like text formats . Um , b you can always , uh , G - zip them , and , um , you know , c decompress them on the fly if y if space is really a concern . Yeah , I was thi I was thinking the advantage is that we can share this with other people . Well , but if you 're talking about one per frame , you 're talking about gigabyte - size files . You 're gonna actually run out of space in your filesystem for one file . These are big files . These are really {disfmarker} I mean {disfmarker} Right ? Because you have a two - gigabyte limit on most O Ss . Right , OK . I would say {disfmarker} OK , so frame - level is probably not a good idea . But for phone - level stuff it 's perfectly {disfmarker} And th it 's {disfmarker} Like phones , or syllables , or anything like that . Phones are every five frames though , so . Or something like that . But {disfmarker} but {disfmarker} but most of the frames are actually not speech . So , you know , people don't {disfmarker} v Look at it , words times the average {disfmarker} The average number of phones in an English word is , I don't know , {comment} five maybe ? Yeah , but we actually {disfmarker} So , look at it , t number of words times five . That 's not {disfmarker} that not {disfmarker} Oh , so you mean pause phones take up a lot of the {disfmarker} long pause phones . Exactly . Yep . Yeah . Yeah . OK . That 's true . But you do have to keep them in there . Y yeah . So I think it {disfmarker} it 's debatable whether you want to do phone - level in the same thing . OK . But I think , a anything at frame - level , even P - file , is too verbose . OK . So {disfmarker} I would use something tighter than P - files . Do you {disfmarker} Are you familiar with it ? So . I haven't seen this particular format , I mean , I 've {disfmarker} I 've used them . but {disfmarker} I don't know what their structure is . OK . I 've forgot what the str But , wait a minute , P - file for each frame is storing a vector of cepstral or PLP values , It 's whatever you want , actually . right ? Right . So that {disfmarker} what 's nice about the P - file {disfmarker} It {disfmarker} i Built into it is the concept of {pause} frames , utterances , sentences , that sort of thing , that structure . And then also attached to it is an arbitrary vector of values . And it can take different types . Oh . So it {disfmarker} th they don't all have to be floats . You know , you can have integers and you can have doubles , and all that sort of stuff . So that {disfmarker} that sounds {disfmarker} that sounds about what I w Um . Right ? And it has a header {disfmarker} it has a header format that {pause} describes it {pause} to some extent . So , the only problem with it is it 's actually storing the {pause} utterance numbers and the {pause} frame numbers in the file , even though they 're always sequential . And so it does waste a lot of space . Hmm . But it 's still a lot tighter than {disfmarker} than ASCII . And we have a lot of tools already to deal with it . You do ? OK . Is there some documentation on this somewhere ? Yeah , there 's a ton of it . Man - pages and , uh , source code , and me . OK , great . So , I mean , that sounds good . I {disfmarker} I was just looking for something {disfmarker} I 'm not a database person , but something sort of standard enough that , you know , if we start using this we can give it out , other people can work on it , Yeah , it 's not standard . or {disfmarker} {comment} Is it {disfmarker} ? I mean , it 's something that we developed at ICSI . But , uh {disfmarker} But it 's {pause} been used here But it 's been used here and people 've {disfmarker} and {disfmarker} and , you know , we have a {pause} well - configured system that you can distribute for free , and {disfmarker} I mean , it must be the equivalent of whatever you guys used to store feat your computed features in , right ? OK . Yeah , th we have {disfmarker} Actually , we {disfmarker} we use a generalization of the {disfmarker} the Sphere format . Mmm . Um , but {disfmarker} Yeah , so there is something like that but it 's , um , probably not as sophist Well , what does H T K do for features ? And I think there 's {disfmarker} Or does it even have a concept of features ? They ha it has its own {disfmarker} I mean , Entropic has their own feature format that 's called , like , S - SD or some so SF or something like that . Yeah . I 'm just wondering , would it be worth while to use that instead ? Yeah . Hmm ? Yeah . Th - this is exactly the kind of decision {disfmarker} It 's just whatever {disfmarker} But , I mean , people don't typically share this kind of stuff , right ? Right . They generate their own . I mean {disfmarker} Yeah . Actually , I {disfmarker} I just {disfmarker} you know , we {disfmarker} we 've done this stuff on prosodics and three or four places have asked for those prosodic files , and we just have an ASCII , uh , output of frame - by - frame . Ah , right . Which is fine , but it gets unwieldy to go in and {disfmarker} and query these files with really huge files . Right . I mean , we could do it . I was just thinking if there 's something that {disfmarker} where all the frame values are {disfmarker} And a and again , if you have a {disfmarker} if you have a two - hour - long meeting , that 's gonna {disfmarker} Hmm ? They 're {disfmarker} they 're fair they 're quite large . Yeah , I mean , they 'd be emo enormous . And these are for ten - minute Switchboard conversations , Right . and {disfmarker} So it 's doable , it 's just that you can only store a feature vector at frame - by - frame and it doesn't have any kind of , Is {disfmarker} is the sharing part of this a pretty important {pause} consideration um {disfmarker} or does that just sort of , uh {disfmarker} a nice thing to have ? I {disfmarker} I don't know enough about what we 're gonna do with the data . But I thought it would be good to get something that we can {disfmarker} that other people can use or adopt for their own kinds of encoding . And just , I mean we have to use some we have to make some decision about what to do . Yeah . And especially for the prosody work , what {disfmarker} what it ends up being is you get features from the signal , and of course those change every time your alignments change . So you re - run a recognizer , you want to recompute your features , um , and then keep the database up to date . Right . Or you change a word , or you change a {vocalsound} utterance boundary segment , which is gonna happen a lot . And so I wanted something where {pause} all of this can be done in a elegant way and that if somebody wants to try something or compute something else , that it can be done flexibly . Um , it doesn't have to be pretty , it just has to be , you know , easy to use , and {disfmarker} Yeah , the other thing {disfmarker} We should look at ATLAS , the NIST thing , Oh . Mmm . and see if they have anything at that level . Uh {disfmarker} I mean , I 'm not sure what to do about this with ATLAS , because they chose a different route . I chose something that {disfmarker} Th - there are sort of two choices . Your {disfmarker} your file format can know about {disfmarker} know that you 're talking about language {pause} and speech , which is what I chose , and time , or your file format can just be a graph representation . And then the application has to impose the structure on top . So what it looked like ATLAS chose is , they chose the other way , which was their file format is just nodes and links , and you have to interpret what they mean yourself . And why did you not choose that type of approach ? Uh , because I knew that we were doing speech , and I thought it was better if you 're looking at a raw file to be {disfmarker} t for the tags to say \" it 's an utterance \" , as opposed to the tag to say \" it 's a link \" . OK . OK . So , but {disfmarker} But other than that , are they compatible ? I mean , you could sort of {disfmarker} Yeah , they 're reasonably compatible . I mean , you {disfmarker} you could {disfmarker} You could probably translate between them . Yep . Yeah , that 's w So , So , well , the other thing is if we choose to use ATLAS , which maybe we should just do , we should just throw this out before we invest a lot of time in it . OK . I don't {disfmarker} So this is what the meeting 's about , Yeah . just sort of how to {disfmarker} Um , cuz we need to come up with a database like this just to do our work . And I actually don't care , as long as it 's something useful to other people , what we choose . Yeah . So maybe it 's {disfmarker} maybe oth you know , if {disfmarker} if you have any idea of how to choose , cuz I don't . The only thing {disfmarker} Yeah . Do they already have tools ? I mean , I {disfmarker} I chose this for a couple reasons . One of them is that it 's easy to parse . You don't need a full XML parser . It 's very easy to just write a Perl script {pause} to parse it . As long as uh each tag is on one line . Exactly . Exactly . Which I always do . And you can have as much information in the tag as you want , right ? Well , I have it structured . Right ? So each type tag has only particular items that it can take . Can you {disfmarker} But you can add to those structures if you {disfmarker} Sure . If you have more information . So what {disfmarker} What NIST would say is that instead of doing this , you would say something like \" link {nonvocalsound} start equals , um , you know , some node ID , Yeah . So {disfmarker} end equals some other node ID \" , and then \" type \" would be \" utterance \" . Hmm . You know , so it 's very similar . So why would it be a {disfmarker} a waste to do it this way if it 's similar enough that we can always translate it ? It probably wouldn't be a waste . It would mean that at some point if we wanted to switch , we 'd just have to translate everything . Write a translator . But it se Since they are developing a big {disfmarker} But it {disfmarker} but that sounds {disfmarker} But that 's {disfmarker} I don't think that 's a big deal . As long as it is {disfmarker} they 're developing a big infrastructure . And so it seems to me that if {disfmarker} if we want to use that , we might as well go directly to what they 're doing , rather than {disfmarker} If we want to {disfmarker} Do they already have something that 's {disfmarker} that would be useful for us in place ? Yeah . See , that 's the question . I mean , how stable is their {disfmarker} Are they ready to go , The {disfmarker} I looked at it {disfmarker} or {disfmarker} ? The last time I looked at it was a while ago , probably a year ago , uh , when we first started talking about this . Hmm . And at that time at least {vocalsound} it was still not very {pause} complete . And so , specifically they didn't have any external format representation at that time . They just had the sort of conceptual {pause} node {disfmarker} uh , annotated transcription graph , which I really liked . And that 's exactly what this stuff is based on . Since then , they 've developed their own external file format , which is , uh , you know , this sort of s this sort of thing . Um , and apparently they 've also developed a lot of tools , but I haven't looked at them . Maybe I should . We should {disfmarker} we should find out . I mean , would the tools {disfmarker} would the tools run on something like this , if you can translate them anyway ? Um , th what would {disfmarker} would {disfmarker} would {disfmarker} what would worry me is that maybe we might miss a little detail It 's a hassle I mean , that {disfmarker} I guess it 's a question that {disfmarker} if {disfmarker} uh , yeah . that would make it very difficult to translate from one to the other . OK . I {disfmarker} I think if it 's conceptually close , and they already have or will have tools that everybody else will be using , I mean , {vocalsound} it would be crazy to do something s you know , separate that {disfmarker} OK . Yeah , we might as well . Yep . Yeah . So I 'll {disfmarker} I 'll take a closer look at it . Actually , so it 's {disfmarker} that {disfmarker} that would really be the question , is just what you would feel is in the long run the best thing . And {disfmarker} Right . Cuz {vocalsound} once we start , sort of , doing this I don't {disfmarker} we don't actually have enough time to probably have to rehash it out again The {disfmarker} Yep . The other thing {disfmarker} the other way that I sort of established this was as easy translation to and from the Transcriber format . and {disfmarker} s Right . Um , Right . but {disfmarker} I mean , I like this . This is sort of intuitively easy to actually r read , Yep . as easy it could {disfmarker} as it could be . But , I suppose that {pause} as long as they have a type here that specifies \" utt \" , um , It 's almost the same . it 's {disfmarker} yeah , close enough that {disfmarker} The {disfmarker} the {disfmarker} the {disfmarker} the point is {disfmarker} with this , though , is that you can't really add any supplementary information . Right ? So if you suddenly decide that you want {disfmarker} You have to make a different type . Yeah . You 'd have to make a different type . So {disfmarker} Well , if you look at it and {disfmarker} Um , I guess in my mind I don't know enough {disfmarker} Jane would know better , {comment} about the {pause} types of annotations and {disfmarker} and {disfmarker} But I imagine that those are things that would {disfmarker} well , you guys mentioned this , {comment} that could span any {disfmarker} it could be in its own channel , it could span time boundaries of any type , Right . it could be instantaneous , things like that . Um , and then from the recognition side we have backtraces at the phone - level . Right . If {disfmarker} if it can handle that , it could handle states or whatever . And then at the prosody - level we have frame {disfmarker} sort of like cepstral feature files , Yep . uh , like these P - files or anything like that . And that 's sort of the world of things that I {disfmarker} And then we have the aligned channels , of course , Right . It seems to me you want to keep the frame - level stuff separate . and {disfmarker} Yeah . And then {disfmarker} I {disfmarker} I definitely agree and I wanted to find actually a f a nicer format or a {disfmarker} maybe a more compact format than what we used before . Right . Just cuz you 've got {vocalsound} ten channels or whatever and two hours of a meeting . It 's {disfmarker} it 's a lot of {disfmarker} Huge . Now {disfmarker} now how would you {disfmarker} how would you represent , um , multiple speakers in this framework ? Were {disfmarker} You would just represent them as {disfmarker} Um , You would have like a speaker tag or something ? there 's a spea speaker tag up at the top which identifies them and then each utt the way I had it is each turn or each utterance , {comment} I don't even remember now , had a speaker ID tag attached to it . Mm - hmm . OK . And in this format you would have a different tag , which {disfmarker} which would , uh , be linked to the link . So {disfmarker} so somewhere else you would have another thing {pause} that would be , Yeah . um {disfmarker} Let 's see , would it be a node or a link ? Um {disfmarker} And so {disfmarker} so this one would have , um , an ID is link {disfmarker} {comment} link seventy - four or something like that . Mm - hmm . And then somewhere up here you would have a link that {disfmarker} that , uh , you know , was referencing L - seventy - four and had speaker Adam . Is i ? You know , or something like that . Actually , it 's the channel , I think , that {disfmarker} Well , channel or speaker or whatever . I mean , w yeah , channel is what the channelized output out It doesn't {disfmarker} This isn't quite right . Right . I have to look at it again . Yeah , but {disfmarker} But {disfmarker} but {disfmarker} so how in the NIST format do we express {vocalsound} a hierarchical relationship between , um , say , an utterance and the words within it ? So how do you {pause} tell {pause} that {pause} these are the words that belong to that utterance ? Um , you would have another structure lower down than this that would be saying they 're all belonging to this ID . Mm - hmm . So each thing refers to the {pause} utterance that it belongs to . Right . And then each utterance could refer to a turn , So it 's {disfmarker} it 's not hi it 's sort of bottom - up . and each turn could refer to something higher up . And what if you actually have {disfmarker} So right now what you have as utterance , um , the closest thing that comes out of the channelized is the stuff between the segment boundaries that the transcribers put in or that Thilo put in , which may or may not actually be , like , a s it 's usually not {disfmarker} um , the beginning and end of a sentence , say . Well , that 's why I didn't call it \" sentence \" . So , right . Um , so it 's like a segment or something . Yeah . So , I mean , I assume this is possible , that if you have {disfmarker} someone annotates the punctuation or whatever when they transcribe , you can say , you know , from {disfmarker} for {disfmarker} from the c beginning of the sentence to the end of the sentence , from the annotations , this is a unit , even though it never actually {disfmarker} i It 's only a unit by virtue of the annotations {pause} at the word - level . Sure . I mean , so you would {disfmarker} you would have yet another tag . And then that would get a tag somehow . You 'd have another tag which says this is of type \" sentence \" . OK . OK . And , what {disfmarker} But it 's just not overtly in the {disfmarker} OK . Um , cuz this is exactly the kind of {disfmarker} So {disfmarker} I think that should be {pause} possible as long as the {disfmarker} But , uh , what I don't understand is where the {disfmarker} where in this type of file {pause} that would be expressed . Right . You would have another tag somewhere . It 's {disfmarker} well , there 're two ways of doing it . S so it would just be floating before the sentence or floating after the sentence without a time - mark . You could have some sort of link type {disfmarker} type equals \" sentence \" , and ID is \" S - whatever \" . And then lower down you could have an utterance . So the type is \" utterance \" {disfmarker} equals \" utt \" . And you could either say that {disfmarker} No . I don't know {disfmarker} So here 's the thing . I take that back . Um {disfmarker} Can you {disfmarker} can you say that this is part of this , See , cuz it 's {disfmarker} Hhh . it 's {disfmarker} You would just have a r S or do you say this is part of this ? I think {disfmarker} You would refer up to the sentence . But they 're {disfmarker} Well , the thing {disfmarker} they 're actually overlapping each other , sort of . So {disfmarker} the thing is that some something may be a part of one thing for one purpose and another thing of another purpose . Right . So f You have to have another type then , I guess . s Um , well , s let 's {disfmarker} let 's ta so let 's {disfmarker} Well , I think I 'm {disfmarker} I think w I had better look at it again Yeah . so {disfmarker} because I {disfmarker} I 'm {disfmarker} OK . OK . y So for instance @ @ {comment} sup There 's one level {disfmarker} there 's one more level of indirection that I 'm forgetting . Suppose you have a word sequence and you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences . And another segmentation is in terms of , um , {vocalsound} I don't know , {comment} prosodic phrases . And let 's say that they don't {pause} nest . So , you know , a prosodic phrase may cross two sentences or something . Right . I don't know if that 's true or not but {vocalsound} let 's as Well , it 's definitely true with the segment . Right . That 's what I {disfmarker} exactly what I meant by the utterances versus the sentence could be sort of {disfmarker} Yeah . So , you want to be s you want to say this {disfmarker} this word is part of that sentence and this prosodic phrase . Yeah . But the phrase is not part of the sentence Yeah . and neither is the sentence part of the phrase . Right . I I 'm pretty sure that you can do that , but I 'm forgetting the exact level of nesting . So , you would have to have {vocalsound} two different pointers from the word up {disfmarker} one level up , one to the sent So {disfmarker} so what you would end up having is a tag saying \" here 's a word , and it starts here and it ends here \" . Right . And then lower down you would say \" here 's a prosodic boundary and it has these words in it \" . And lower down you 'd have \" here 's a sentence , Right . An - Right . and it has these words in it \" . So you would be able to go in and say , you know , \" give me all the words in the bound in the prosodic phrase Yep . and give me all the words in the {disfmarker} \" Yeah . So I think that 's {disfmarker} that would wor Um , OK . Let me look at it again . Mm - hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmarker} find and extract information in a structure of this type ? OK . So . That 's good . So you gave some examples like {disfmarker} Well , uh , and , I mean , you guys might {disfmarker} I don't know if this is premature because I suppose once you get the representation you can do this , but the kinds of things I was worried about is , No , that 's not clear . uh {disfmarker} I mean , yeah , you c sure you can do it , Well , OK . So i if it {disfmarker} but can you do it sort of l l you know , it {disfmarker} I I mean , I can't do it , but I can {disfmarker} um , y y you gotta {disfmarker} you gotta do this {disfmarker} you {disfmarker} you 're gonna want to do this very quickly Well {disfmarker} or else you 'll spend all your time sort of searching through very {vocalsound} complex data structures {disfmarker} Right . You 'd need a p sort of a paradigm for how to do it . But an example would be \" find all the cases in which Adam started to talk while Andreas was talking and his pitch was rising , Andreas 's pitch \" . That kind of thing . Right . I mean , that 's gonna be {disfmarker} Is the rising pitch a {pause} feature , or is it gonna be in the same file ? Well , the rising pitch will never be {pause} hand - annotated . So the {disfmarker} all the prosodic features are going to be automatically {disfmarker} But the {disfmarker} I mean , that 's gonna be hard regardless , So they 're gonna be in those {disfmarker} right ? Because you 're gonna have to write a program that goes through your feature file and looks for rising pitches . Yeah . So {disfmarker} Right . So normally what we would do is we would say \" what do we wanna assign rising pitch to ? \" Are we gonna assign it to words ? Are we gonna just assign it to sort of {disfmarker} when it 's rising we have a begin - end rise representation ? But suppose we dump out this file and we say , uh , for every word we just classify it as , w you know , rise or fall or neither ? OK . Well , in that case you would add that to this {pause} format OK . r So we would basically be sort of , um , taking the format and enriching it with things that we wanna query in relation to the words that are already in the file , Right . and then querying it . You want sort of a grep that 's {disfmarker} that works at the structural {disfmarker} on the structural representation . OK . You have that . There 's a {pause} standard again in XML , specifically for searching XML documents {disfmarker} structured X - XML documents , where you can specify both the content and the structural position . Yeah , but it 's {disfmarker} it 's not clear that that 's {disfmarker} That 's relative to the structure of the XML document , If {disfmarker} not to the structure of what you 're representing in the document . You use it as a tool . You use it as a tool , not an end - user . It 's not an end - user thing . Right . It 's {disfmarker} it 's {disfmarker} you would use that to build your tool to do that sort of search . Right . Be Because here you 're specifying a lattice . Uh {disfmarker} So the underlying {disfmarker} that 's the underlying data structure . And you want to be able to search in that lattice . But as long as the {disfmarker} It 's a graph , but {disfmarker} That 's different from searching through the text . But it seems like as long as the features that {disfmarker} Well , no , no , no . The whole point is that the text and the lattice are isomorphic . They {pause} represent each other {pause} completely . Um {disfmarker} So that {disfmarker} I mean th That 's true if the features from your acoustics or whatever that are not explicitly in this are at the level of these types . Hhh . That {disfmarker} that if you can do that {disfmarker} Yeah , but that 's gonna be the trouble no matter what . Right ? No matter what format you choose , you 're gonna have the trou you 're gonna have the difficulty of relating the {disfmarker} the frame - level features {disfmarker} That 's right . That 's true . That 's why I was trying to figure out what 's the best format for this representation . Yep . And it 's still gonna be {disfmarker} Hmm . it 's still gonna be , uh , not direct . Right . You know , it {disfmarker} Or another example was , you know , uh , where in the language {disfmarker} where in the word sequence are people interrupting ? So , I guess that one 's actually easier . What about {disfmarker} what about , um , the idea of using a relational database to , uh , store the information from the XML ? So you would have {disfmarker} XML basically would {disfmarker} Uh , you {disfmarker} you could use the XML to put the data in , and then when you get data out , you put it back in XML . So use XML as sort of the {disfmarker} the transfer format , Transfer . uh , but then you store the data in the database , which allows you to do all kinds of {pause} good search things in there . The , uh {disfmarker} One of the things that ATLAS is doing is they 're trying to define an API which is independent of the back store , Huh . so that , uh , you could define a single API and the {disfmarker} the storage could be flat XML files or a database . Mm - hmm . My opinion on that is for the s sort of stuff that we 're doing , {comment} I suspect it 's overkill to do a full relational database , that , um , just a flat file and , uh , search tools I bet will be enough . But {disfmarker} But that 's the advantage of ATLAS , is that if we actually take {disfmarker} decide to go that route completely and we program to their API , then if we wanted to add a database later it would be pretty easy . Mm - hmm . Mm - hmm . It seems like the kind of thing you 'd do if {disfmarker} I don't know , if people start adding all kinds of s bells and whistles to the data . And so that might be {disfmarker} I mean , it 'd be good for us to know {disfmarker} to use a format where we know we can easily , um , input that to some database if other people are using it . Yep . Something like that . I guess I 'm just a little hesitant to try to go whole hog on sort of the {disfmarker} the whole framework that {disfmarker} that NIST is talking about , with ATLAS and a database and all that sort of stuff , So {disfmarker} cuz it 's a big learning curve , just to get going . Hmm . Hmm . Whereas if we just do a flat file format , sure , it may not be as efficient but everyone can program in Perl and {disfmarker} and use it . OK . Right ? But this is {disfmarker} So , as opposed to {disfmarker} I {disfmarker} I 'm still , um , {vocalsound} not convinced that you can do much at all on the text {disfmarker} on the flat file that {disfmarker} that {disfmarker} you know , the text representation . e Because the text representation is gonna be , uh , not reflecting the structure of {disfmarker} of your words and annotations . It 's just {disfmarker} it 's {disfmarker} Well , if it 's not representing it , then how do you recover it ? Of course it 's representing it . No . You {disfmarker} you have to {disfmarker} what you have to do is you have to basically {disfmarker} That 's the whole point . Y yeah . You can use Perl to read it in and construct a internal representation that is essentially a lattice . But , the {disfmarker} and then {disfmarker} OK . Yeah . Well , that was a different point . Right . Right ? So what I was saying is that {disfmarker} But that 's what you 'll have to do . Bec - be For Perl {disfmarker} if you want to just do Perl . If you wanted to use the structured XML query language , that 's a different thing . And it 's a set of tools {vocalsound} that let you specify given the D - DDT {disfmarker} DTD of the document , um , what sorts of structural searches you want to do . So you want to say that , you know , you 're looking for , um , a tag within a tag within a particular tag that has this particular text in it , um , and , uh , refers to a particular value . And so the point isn't that an end - user , who is looking for a query like you specified , wouldn't program it in this language . What you would do is , someone would build a tool that used that as a library . So that they {disfmarker} so that you wouldn't have to construct the internal representations yourself . Is a {disfmarker} See , I think the kinds of questions , at least in the next {disfmarker} to the end of this year , are {disfmarker} there may be a lot of different ones , but they 'll all have a similar nature . They 'll be looking at either a word - level prosodic , uh , an {disfmarker} a value , Mm - hmm . like a continuous value , like the slope of something . But you know , we 'll do something where we {disfmarker} some kind of data reduction where the prosodic features are sort o uh , either at the word - level or at the segment - level , Right . or {disfmarker} or something like that . They 're not gonna be at the phone - level and they 're no not gonna be at the frame - level when we get done with sort of giving them simpler shapes and things . And so the main thing is just being able {disfmarker} Well , I guess , the two goals . Um , one that Chuck mentioned is starting out with something that we don't have to start over , that we don't have to throw away if other people want to extend it for other kinds of questions , Right . and being able to at least get enough , uh , information out on {disfmarker} where we condition the location of features on information that 's in the kind of file that you {pause} put up there . And that would {disfmarker} that would do it , Yeah . I think that there are quick and dirty solutions , I mean , for me . and then there are long - term , big - infrastructure solutions . And so {vocalsound} we want to try to pick something that lets us do a little bit of both . In the between , right . And especially that the representation doesn't have to be thrown away , Um {disfmarker} Right . even if your tools change . And so it seems to me that {disfmarker} I mean , I have to look at it again to see whether it can really do what we want , but if we use the ATLAS external file representation , um , it seems like it 's rich enough that you could do quick tools just as I said in Perl , and then later on if we choose to go up the learning curve , we can use the whole ATLAS inter infrastructure , Yeah . I mean , that sounds good to me . which has all that built in . I {disfmarker} I don't {disfmarker} So if {disfmarker} if you would l look at that and let us know what you think . Sure . I mean , I think we 're sort of guinea pigs , cuz I {disfmarker} I want to get the prosody work done but I don't want to waste time , you know , getting the {disfmarker} Oh , maybe {disfmarker} Yeah ? um {disfmarker} Well , I wouldn't wait for the formats , because anything you pick we 'll be able to translate to another form . Well {disfmarker} Ma well , maybe you should actually look at it yourself too to get a sense of what it is you 'll {disfmarker} you 'll be dealing with , OK . because , um , you know , Adam might have one opinion but you might have another , so Yeah . Yeah , definitely . I think the more eyes look at this the better . Especially if there 's , e um {disfmarker} you know , if someone can help with at least the {disfmarker} the setup of the right {disfmarker} Hi , Jane . Oh , hi . Mmm . the right representation , then , i you know , I hope it won't {disfmarker} We don't actually need the whole full - blown thing to be ready , Can you {disfmarker} Oh , well . so . Um , so maybe if you guys can look at it and sort of see what , Yeah . Sure . um {disfmarker} I think we 're {disfmarker} we 're {disfmarker} {vocalsound} we 're actually just {disfmarker} We 're about done . yeah , Hmm . wrapping up , but , um {disfmarker} Yeah , sorry , it 's a uh short meeting , but , um {disfmarker} Well , I don't know . Is there anything else , like {disfmarker} I mean that helps me a lot , Well , I think the other thing we might want to look at is alternatives to P - file . but {disfmarker} I mean , th the reason I like P - file is I 'm already familiar with it , we have expertise here , and so if we pick something else , there 's the learning - curve problem . But , I mean , it is just something we developed at ICSI . Is there an {disfmarker} is there an IP - API ? And so {disfmarker} Yeah . OK . There 's an API for it . And , uh , There used to be a problem that they get too large , a bunch of libraries , P - file utilities . and so {pause} basically the {disfmarker} uh the filesystem wouldn't {disfmarker} Well , that 's gonna be a problem no matter what . You have the two - gigabyte limit on the filesystem size . And we definitely hit that with Broadcast News . Maybe you could extend the API to , uh , support , uh , like splitting up , you know , conceptually one file into smaller files on disk so that you can essentially , you know , have arbitrarily long f Yep . Most of the tools can handle that . Yeah . So that we didn't do it at the API - level . We did it at the t tool - level . That {disfmarker} that {disfmarker} most {disfmarker} many of them can s you can specify several P - files and they 'll just be done sequentially . OK . So . So , I guess , yeah , if {disfmarker} if you and Don can {disfmarker} if you can show him the P - file stuff and see . Sure . So this would be like for the F - zero {disfmarker} True . I mean , if you do \" man P - file \" or \" apropos P - file \" , you 'll see a lot . I 've used the P - file , I think . I 've looked at it at least , briefly , I think when we were doing s something . What does the P stand for anyway ? I have no idea . Oh , in there . I didn't de I didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quicknet library . It has all the utilities for it . No , P - files were around way before Quicknet . P - files were {disfmarker} were around when {disfmarker} w with , um , {vocalsound} RAP . Oh , were they ? Mm - hmm . Right ? It 's like the history of ICSI . You worked with P - files . Mm - hmm . Like {disfmarker} No . I worked with P - files . Yeah ? I don't remember what the \" P \" is , though . No . But there are ni they 're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P - files , Yeah . so it works pretty well .  And that isn't really , I guess , as important as the {disfmarker} the main {disfmarker} I don't know what you call it , the {disfmarker} the main sort of word - level {disfmarker} Neither do I . Probably stands for \" Phil \" . Phil Kohn . It 's a Phil file ? Yeah . That 's my guess . Huh . OK . Well , that 's really useful . I mean , this is exactly the kind of thing that I wanted to settle . Um , so {disfmarker} Yeah , I 've been meaning to look at the ATLAS stuff again anyway . Great . So , just keep {disfmarker} Yeah . I guess it 's also sort of a political deci I mean , if {disfmarker} if you feel like that 's a community that would be good to tie into anyway , then it 's {disfmarker} sounds like it 's worth doing . Yeah , I think it {disfmarker} it w j I think there 's {disfmarker} And , w uh , as I said , I {disfmarker} what I did with this stuff {disfmarker} I based it on theirs . It 's just they hadn't actually come up with an external format yet . So now that they have come up with a format , it doesn't {disfmarker} it seems pretty reasonable to use it . Mmm . But let me look at it again . OK , great . As I said , that {disfmarker} Cuz we actually can start {disfmarker} There 's one level {disfmarker} there 's one more level of indirection and I 'm just blanking on exactly how it works . I gotta look at it again . I mean , we can start with , um , I guess , this input from Dave 's , which you had printed out , the channelized input . Cuz he has all of the channels , you know , with the channels in the tag and stuff like that . Yeah , I 've seen it . So that would be i directly , Yep . Easy {disfmarker} easy to map . um {disfmarker} Yeah . And so then it would just be a matter of getting {disfmarker} making sure to handle the annotations that are , you know , not at the word - level and , um , t to import the Where are those annotations coming from ? Well , right now , I g Jane would {disfmarker} {vocalsound} would {disfmarker} Mm - hmm . Yeah . Are you talking about the overlap a annotations ? Yeah , any kind of annotation {pause} that , like , isn't already there . Uh , you know , anything you can envision . Yeah . So what I was imagining was {disfmarker} um , so Dave says we can have unlimited numbers of green ribbons . And so put , uh , a {disfmarker} a green ribbon on for an overlap code . And since we w we {disfmarker} I {disfmarker} I think it 's important to remain flexible regarding the time bins for now . And so it 's nice to have {disfmarker} However , you know , you want to have it , uh , time time uh , located in the discourse . So , um , if we {disfmarker} if we tie the overlap code to the first word in the overlap , then you 'll have a time - marking . It won't {disfmarker} it 'll be independent of the time bins , however these e evolve , shrink , or whatever , increase , or {disfmarker} Also , you could have different time bins for different purposes . And having it tied to the first word in an overlap segment is unique , uh , you know , anchored , clear . And it would just end up on a separate ribbon . Right . So the overlap coding is gonna be easy with respect to that . You look puzzled . I {disfmarker} I just {disfmarker} I don't quite understand what these things are . OK . Uh . What , the codes themselves ? Well , th overlap codes . Or the {disfmarker} ? I 'm not sure what that @ @ {disfmarker} Well , I mean , is that {disfmarker} It probably doesn't matter . Well , we don't have to go into the codes . I mean , it doesn't . No , I d We don't have to go into the codes . I mean , that {disfmarker} not for the topic of this meeting . But let me just {disfmarker} No . W the idea is just to have a separate green ribbon , you know , and {disfmarker} and {disfmarker} and let 's say that this is a time bin . There 's a word here . This is the first word of an overlapping segment of any length , overlapping with any other , uh , word {disfmarker} uh , i segment of any length . And , um , then you can indicate that this here was perhaps a ch a backchannel , or you can say that it was , um , a usurping of the turn , or you can {disfmarker} you know , any {disfmarker} any number of categories . But the fact is , you have it time - tagged in a way that 's independent of the , uh , sp particular time bin that the word ends up in . If it 's a large unit or a small unit , or Mm - hmm . we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format , Right . flexible , all that . Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ? Yeah . And sometimes the word sequences even differ slightly because they were edited s at one place but not the other . Yeah . So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what the heck . You know , there 's a zillion things that people might annotate this for . And the only thing that is really sort of common among all the versi the various versions of this data is the word sequence , or approximately . Yep . Or the time . Or the times . But , see , if you 'd annotate dialogue acts , you don't necessarily want to {disfmarker} or topics {disfmarker} you don't really want to be dealing with time - marks . I guess . You 'd {disfmarker} it 's much more efficient for them to just see the word sequence , right ? Mm - hmm . I mean , most people aren't as sophisticated as {disfmarker} as we are here with , you know , uh , time alignments and stuff . So {disfmarker} So the {disfmarker} the {disfmarker} the point is {disfmarker} Should {disfmarker} should we mention some names on the people who are n ? Right . So , um , the p my point is that {pause} you 're gonna end up with , uh , word sequences that are differently annotated . And {pause} you want some tool , uh , that is able to sort of merge these different annotations back into a single , uh , version . OK ? Um , and we had this problem very massively , uh , at SRI when we worked , uh , a while back on , {vocalsound} uh {disfmarker} well , on dialogue acts as well as , uh , you know , um , what was it ? uh , Well , all the Switchboard in it . utterance types . There 's , uh , automatic , uh , punctuation and stuff like that . Yeah . Because we had one set of {pause} annotations that were based on , uh , one version of the transcripts with a particular segmentation , and then we had another version that was based on , uh , a different s slightly edited version of the transcripts with a different segmentation . So , {vocalsound} we had these two different versions which were {disfmarker} you know , you could tell they were from the same source but they weren't identical . So it was extremely hard {vocalsound} to reliably merge these two back together to correlate the information from the different annotations . Yep . I {disfmarker} I don't see any way that file formats are gonna help us with that . No . It 's {disfmarker} it 's all a question of semantic . No . But once you have a file format , I can imagine writing {disfmarker} not personally , but someone writing a tool that is essentially an alignment tool , um , that mediates between various versions , Mm - hmm . Yeah . and {disfmarker} uh , sort of like th uh , you know , you have this thing in UNIX where you have , uh , diff . Diff . W - diff or diff . There 's the , uh , diff that actually tries to reconcile different {disfmarker} two diffs f {comment} based on the same original . Yeah . Is it S - diff ? Yep . Mmm . Something like that , um , but operating on these lattices that are really what 's behind this {disfmarker} uh , this annotation format . Yep . So {disfmarker} There 's actually a diff library you can use {pause} to do things like that that {disfmarker} so you have different formats . You could definitely do that with the {disfmarker} So somewhere in the API you would like to have like a merge or some {disfmarker} some function that merges two {disfmarker} two versions . Yeah , I think it 's gonna be very hard . Any sort of structured anything when you try to merge is really , really hard Right . because you ha i The hard part isn't the file format . The hard part is specifying what you mean by \" merge \" . Is {disfmarker} Exactly . And that 's very difficult . But the one thing that would work here actually for i that is more reliable than the utterances is the {disfmarker} the speaker ons and offs . So if you have a good , But this is exactly what I mean , is that {disfmarker} that the problem i um {disfmarker} Yeah . You just have to know wha what to tie it to . Yeah , exactly . The problem is saying \" what are the semantics , And {disfmarker} what do you mean by \" merge \" ? \" Right , right . Right . So {disfmarker} so just to let you know what we {disfmarker} where we kluged it by , uh , doing {disfmarker} uh , by doing {disfmarker} Hhh . So . Both were based on words , so , bo we have two versions of the same words intersp you know , sprinkled with {disfmarker} with different tags for annotations . And then you did diff . And we did diff . Exactly ! Yeah , that 's just what I thought . And that 's how {disfmarker} That 's just wh how I would have done it . Yeah . But , you know , it had lots of errors and things would end up in the wrong order , and so forth . Uh , so , um , if you had a more {disfmarker} Yep . Uh , it {disfmarker} it was a kluge because it was basically reducing everything to {disfmarker} uh , to {disfmarker} uh , uh , to textual alignment . A textual {disfmarker} Um , so {disfmarker} But , d isn't that something where whoever {disfmarker} if {vocalsound} {disfmarker} if the people who are making changes , say in the transcripts , cuz this all happened when the transcripts were different {disfmarker} ye um , if they tie it to something , like if they tied it to the acoustic segment {disfmarker} if they {disfmarker} You know what I mean ? Then {disfmarker} Or if they tied it to an acoustic segment and we had the time - marks , that would help . Yep . But the problem is exactly as Adam said , that you get , you know , y you don't have that information or it 's lost in the merge somehow , Well , can I ask one question ? so {disfmarker} It {disfmarker} it seems to me that , um , we will have o an official version of the corpus , which will be only one {disfmarker} one version in terms of the words {disfmarker} where the words are concerned . We 'd still have the {disfmarker} the merging issue maybe if coding were done independently of the {disfmarker} And you 're gonna get that But {disfmarker} but {disfmarker} because if the data gets out , people will do all kinds of things to it . And , uh , s you know , several years from now you might want to look into , um , the prosody of referring expressions . And someone at the university of who knows where has annotated the referring expressions . So you want to get that annotation and bring it back in line with your data . Right . OK ? But unfortunately they 've also hand - edited it . OK , then {disfmarker} But they 've also {disfmarker} Exactly . And so that 's exactly what we should {disfmarker} somehow when you distribute the data , say that {disfmarker} you know , that {disfmarker} have some way of knowing how to merge it back in and asking people to try to do that . Yeah . Yep . Right . Well , then the {disfmarker} What 's {disfmarker} what 's wrong with {pause} doing times ? I {disfmarker} I agree . That was what I was wondering . Uh , yeah , time is the {disfmarker} Well , Time is unique . You were saying that you didn't think we should {disfmarker} Time is passing ! Time {disfmarker} time {disfmarker} times are ephemeral . Andreas was saying {disfmarker} Yeah . what if they haven't notated with them , times ? Yeah . He {disfmarker} he 's a language modeling person , though . Um {disfmarker} So {disfmarker} so imagine {disfmarker} I think his {disfmarker} his example is a good one . Imagine that this person who developed the corpus of the referring expressions didn't include time . Mm - hmm . Yeah . He included references to words . Ach ! Yeah . He said that at this word is when {disfmarker} when it happened . Well , then {disfmarker} Or she . Or she . But then couldn't you just indirectly figure out the time {pause} tied to the word ? But still they {disfmarker} Exactly . Sure . But what if {disfmarker} what if they change the words ? Yeah . Not {disfmarker} Well , but you 'd have some anchoring point . He couldn't have changed all the words . But can they change the words without changing the time of the word ? Sure . But they could have changed it a little . The {disfmarker} the point is , that {disfmarker} that they may have annotated it off a word transcript that isn't the same as our word transcript , so how do you merge it back in ? I understand what you 're saying . Mmm . Mm - hmm . And I {disfmarker} I guess the answer is , um , it 's gonna be different every time . It 's j it 's just gonna be {disfmarker} Yeah . Yeah . I it 's exactly what I said before , You only know the boundaries of the {disfmarker} which is that \" what do you mean by \" merge \" ? \" So in this case where you have the words and you don't have the times , well , what do you mean by \" merge \" ? If you tell me what you mean , I can write a program to do it . Right . Right . You can merge at the level of the representation that the other person preserved and that 's it . Right . And that 's about all you can do . And beyond that , all you know is {disfmarker} is relative ordering and sometimes even that is wrong . So {disfmarker} so in {disfmarker} so in this one you would have to do a best match between the word sequences , So . Mm - hmm . extract the times f from the best match of theirs to yours , and use that . And then infer that their time - marks are somewhere in between . Right . Yeah , exactly . But it could be that they just {disfmarker} uh , I mean , it could be that they chunked {disfmarker} they {disfmarker} they lost certain utterances and all that stuff , Right , exactly . So it could get very , very ugly . or {disfmarker} Definitely . Yeah . Definitely . Alright . That 's interesting . Well , I guess , w I {disfmarker} I didn't want to keep people too long and Adam wanted t people {disfmarker} I 'll read the digits . If anyone else offers to , that 'd be great . And Ah , well . Yeah . if not , I guess {disfmarker} For th for the {disfmarker} {nonvocalsound} for the benefit of science we 'll read the digits . More digits , the better . OK , this is Thanks {disfmarker} thanks a lot . It 's really helpful . I mean , Adam and Don {nonvocalsound} will sort of meet and I think that 's great . Very useful . Go next . Scratch that . O three Oh , right .",
        "summarize": "Meeting participants wanted to agree upon a standard database to link up different components of the transcripts. The current idea was to use an XML script, but it quickly seemed that other options, like a pfile or ATLAS, are more suitable. The reason being that they would make it easier to deal with different linguistic units, like frames and utterances. Eventually, the team was skeptical of using something that would be hard to learn, like ATLAS. Nonetheless, they wanted to explore their options. The meeting finished with some discussion about handling annotations."
    }
]